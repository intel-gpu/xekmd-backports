From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Kanaka Raju Nayana <kanaka.raju.nayana@intel.com>
Date: Thu, 9 Oct 2025 05:36:32 +0530
Subject: [PATCH] drm/xe/pmu: Add manual system-wide PMU and CPU hotplug
 support for older kernels

The perf subsystem in kernels below 6.14 does not support the scope member
in struct pmu, so device-wide PMU events are incorrectly treated as per-CPU.
This change introduces manual logic to restrict event handling to a single
CPU and handle CPU hotplug for system-wide PMU events in the Xe driver.

* Adds a static cpumask and target CPU variable to track the responsible CPU.
* Registers CPU hotplug callbacks to migrate the PMU context when the responsible
  CPU goes offline.
* Restricts perf events to only run on the chosen CPU in xe_pmu_event_init.
* Adds xe_pmu_init() and xe_pmu_exit() for hotplug callback registration and cleanup.

Reference:
    4ba4f1afb6a9
    perf: Generic hotplug support for a PMU with a scope

Co-developed-by: Aravind Iddamsetty <aravind.iddamsetty@linux.intel.com>
Signed-off-by: Aravind Iddamsetty <aravind.iddamsetty@linux.intel.com>
Signed-off-by: Kanaka Raju Nayana <kanaka.raju.nayana@intel.com>
---
 drivers/gpu/drm/xe/xe_module.c    |   7 ++
 drivers/gpu/drm/xe/xe_pmu.c       | 121 +++++++++++++++++++++++++-
 drivers/gpu/drm/xe/xe_pmu.h       |   4 +
 drivers/gpu/drm/xe/xe_pmu_types.h |   7 ++
 4 files changed, 138 insertions(+), 1 deletion(-)

diff --git a/drivers/gpu/drm/xe/xe_module.c b/drivers/gpu/drm/xe/xe_module.c
index 29008ab..966648d 100644
--- a/drivers/gpu/drm/xe/xe_module.c
+++ b/drivers/gpu/drm/xe/xe_module.c
@@ -14,6 +14,7 @@
 #include "xe_hw_fence.h"
 #include "xe_pci.h"
 #include "xe_pm.h"
+#include "xe_pmu.h"
 #include "xe_observation.h"
 #include "xe_sched_job.h"
 
@@ -100,6 +101,12 @@ static const struct init_funcs init_funcs[] = {
 		.init = xe_sched_job_module_init,
 		.exit = xe_sched_job_module_exit,
 	},
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT
+	{
+		.init = xe_pmu_init,
+		.exit = xe_pmu_exit,
+	},
+#endif
 	{
 		.init = xe_register_pci_driver,
 		.exit = xe_unregister_pci_driver,
diff --git a/drivers/gpu/drm/xe/xe_pmu.c b/drivers/gpu/drm/xe/xe_pmu.c
index 69df0e3..9d8decc 100644
--- a/drivers/gpu/drm/xe/xe_pmu.c
+++ b/drivers/gpu/drm/xe/xe_pmu.c
@@ -16,6 +16,105 @@
 #include "xe_pmu.h"
 #include "xe_sriov_pf_helpers.h"
 
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT
+static cpumask_t xe_pmu_cpumask;
+static unsigned int xe_pmu_target_cpu = -1;
+static enum cpuhp_state cpuhp_slot = CPUHP_INVALID;
+
+static ssize_t cpumask_show(struct device *dev,
+					    struct device_attribute *attr, char *buf)
+{
+		return cpumap_print_to_pagebuf(true, buf, &xe_pmu_cpumask);
+}
+
+static DEVICE_ATTR_RO(cpumask);
+
+static struct attribute *xe_cpumask_attrs[] = {
+		&dev_attr_cpumask.attr,
+			NULL,
+};
+
+static const struct attribute_group xe_pmu_cpumask_attr_group = {
+		.attrs = xe_cpumask_attrs,
+};
+
+static int xe_pmu_cpu_online(unsigned int cpu, struct hlist_node *node)
+{
+       struct xe_pmu *pmu = hlist_entry_safe(node, typeof(*pmu), cpuhp.node);
+
+       /* Select the first online CPU as a designated reader. */
+       if (cpumask_empty(&xe_pmu_cpumask))
+               cpumask_set_cpu(cpu, &xe_pmu_cpumask);
+
+       return 0;
+}
+
+static int xe_pmu_cpu_offline(unsigned int cpu, struct hlist_node *node)
+{
+       struct xe_pmu *pmu = hlist_entry_safe(node, typeof(*pmu), cpuhp.node);
+       unsigned int target = xe_pmu_target_cpu;
+
+       if (cpumask_test_and_clear_cpu(cpu, &xe_pmu_cpumask)) {
+               target = cpumask_any_but(topology_sibling_cpumask(cpu), cpu);
+
+               /* Migrate events if there is a valid target */
+               if (target < nr_cpu_ids) {
+                       cpumask_set_cpu(target, &xe_pmu_cpumask);
+                       xe_pmu_target_cpu = target;
+               }
+       }
+
+       if (target < nr_cpu_ids && target != pmu->cpuhp.cpu) {
+               perf_pmu_migrate_context(&pmu->base, cpu, target);
+               pmu->cpuhp.cpu = target;
+       }
+
+       return 0;
+}
+
+int xe_pmu_init(void)
+{
+       int ret;
+
+       cpumask_clear(&xe_pmu_cpumask);
+       xe_pmu_target_cpu = -1;
+
+       ret = cpuhp_setup_state_multi(CPUHP_AP_ONLINE_DYN,
+                                     "perf/x86/intel/xe:online",
+                                     xe_pmu_cpu_online,
+                                     xe_pmu_cpu_offline);
+       if (ret < 0)
+               pr_notice("Failed to setup cpuhp state for xe PMU! (%d)\n",
+                         ret);
+       else
+               cpuhp_slot = ret;
+
+       return 0;
+}
+
+void xe_pmu_exit(void)
+{
+       if (cpuhp_slot != CPUHP_INVALID)
+               cpuhp_remove_multi_state(cpuhp_slot);
+
+       cpumask_clear(&xe_pmu_cpumask);
+       xe_pmu_target_cpu = -1;
+}
+
+static int xe_pmu_register_cpuhp_state(struct xe_pmu *pmu)
+{
+       if (cpuhp_slot == CPUHP_INVALID)
+               return -EINVAL;
+
+       return cpuhp_state_add_instance(cpuhp_slot, &pmu->cpuhp.node);
+}
+
+static void xe_pmu_unregister_cpuhp_state(struct xe_pmu *pmu)
+{
+       cpuhp_state_remove_instance(cpuhp_slot, &pmu->cpuhp.node);
+}
+#endif
+
 /**
  * DOC: Xe PMU (Performance Monitoring Unit)
  *
@@ -245,6 +344,11 @@ static int xe_pmu_event_init(struct perf_event *event)
 	if (event->cpu < 0)
 		return -EINVAL;
 
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT
+	/* only allow running on one cpu at a time */
+	if (!cpumask_test_cpu(event->cpu, &xe_pmu_cpumask))
+		return -EINVAL;
+#endif
 	gt = config_to_gt_id(event->attr.config);
 	id = config_to_event_id(event->attr.config);
 	if (!event_supported(pmu, gt, id))
@@ -520,6 +624,9 @@ static void xe_pmu_unregister(void *arg)
 	if (!pmu->registered)
 		return;
 
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT
+	xe_pmu_unregister_cpuhp_state(pmu);
+#endif
 	pmu->registered = false;
 
 	perf_pmu_unregister(&pmu->base);
@@ -538,6 +645,9 @@ int xe_pmu_register(struct xe_pmu *pmu)
 	static const struct attribute_group *attr_groups[] = {
 		&pmu_format_attr_group,
 		&pmu_events_attr_group,
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT	
+		&xe_pmu_cpumask_attr_group,
+#endif
 		NULL
 	};
 	int ret = -ENOMEM;
@@ -559,7 +669,14 @@ int xe_pmu_register(struct xe_pmu *pmu)
 	pmu->name		= name;
 	pmu->base.attr_groups	= attr_groups;
 	pmu->base.attr_update	= pmu_events_attr_update;
-	pmu->base.scope		= PERF_PMU_SCOPE_SYS_WIDE;
+#ifdef BPM_PMU_SCOPE_MEMBER_NOT_PRESENT
+	pmu->cpuhp.cpu = -1;
+	ret = xe_pmu_register_cpuhp_state(pmu);
+	if (ret)
+		goto err_unreg;
+#else
+	pmu->base.scope         = PERF_PMU_SCOPE_SYS_WIDE;
+#endif
 	pmu->base.module	= THIS_MODULE;
 	pmu->base.task_ctx_nr	= perf_invalid_context;
 	pmu->base.event_init	= xe_pmu_event_init;
@@ -579,6 +696,8 @@ int xe_pmu_register(struct xe_pmu *pmu)
 
 	return devm_add_action_or_reset(xe->drm.dev, xe_pmu_unregister, pmu);
 
+err_unreg:
+	perf_pmu_unregister(&pmu->base);
 err_name:
 	kfree(name);
 err:
diff --git a/drivers/gpu/drm/xe/xe_pmu.h b/drivers/gpu/drm/xe/xe_pmu.h
index 60c3712..2b26cd2 100644
--- a/drivers/gpu/drm/xe/xe_pmu.h
+++ b/drivers/gpu/drm/xe/xe_pmu.h
@@ -9,8 +9,12 @@
 #include "xe_pmu_types.h"
 
 #if IS_ENABLED(CONFIG_PERF_EVENTS)
+int xe_pmu_init(void);
+void xe_pmu_exit(void);
 int xe_pmu_register(struct xe_pmu *pmu);
 #else
+static inline int xe_pmu_init(void) { return 0; }
+static inline void xe_pmu_exit(void) {}
 static inline int xe_pmu_register(struct xe_pmu *pmu) { return 0; }
 #endif
 
diff --git a/drivers/gpu/drm/xe/xe_pmu_types.h b/drivers/gpu/drm/xe/xe_pmu_types.h
index f5ba4d5..efc4ad7 100644
--- a/drivers/gpu/drm/xe/xe_pmu_types.h
+++ b/drivers/gpu/drm/xe/xe_pmu_types.h
@@ -18,6 +18,13 @@
  * counters across all GTs for this device.
  */
 struct xe_pmu {
+	/**
+         * @cpuhp: Struct used for CPU hotplug handling.
+         */
+	struct {
+		struct hlist_node node;
+		unsigned int cpu;
+	} cpuhp;
 	/**
 	 * @base: PMU base.
 	 */
-- 
2.43.0

