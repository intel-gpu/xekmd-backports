From 0000000000000000000000000000000000000000 Mon Sep 17 00:00:00 2001
From: Dominik Grzegorzek <dominik.grzegorzek@intel.com>
Date: Thu, 28 Nov 2024 23:17:11 +0530
Subject: [PATCH 35/35] drm/xe/eudebug: Prelim rework for Xe EU debug

Moved all xe_eudebug declarations and definitions to xe_drm_prelim.h
Removed xe_drm_eudebug.h Moved eudebug related files into prelim/
directory
Added prelim_ prefix where necessary.

V2: Prefix PRELIM using macros instead of file update.

Signed-off-by: Dominik Grzegorzek <dominik.grzegorzek@intel.com>
Signed-off-by: S A Muqthyar Ahmed <syed.abdul.muqthyar.ahmed@intel.com>
---
 drivers/gpu/drm/xe/Kconfig                    |   2 +-
 drivers/gpu/drm/xe/Makefile                   |   6 +-
 .../drm/xe/{ => prelim}/xe_debug_metadata.c   |   0
 drivers/gpu/drm/xe/prelim/xe_debug_metadata.h |  98 ++++
 .../drm/xe/prelim/xe_debug_metadata_types.h   |  25 +
 drivers/gpu/drm/xe/{ => prelim}/xe_eudebug.c  |   0
 drivers/gpu/drm/xe/prelim/xe_eudebug.h        | 106 +++++
 drivers/gpu/drm/xe/prelim/xe_eudebug_types.h  | 449 +++++++++++++++++
 drivers/gpu/drm/xe/{ => prelim}/xe_gt_debug.c |   0
 drivers/gpu/drm/xe/prelim/xe_gt_debug.h       |  42 ++
 drivers/gpu/drm/xe/tests/xe_live_test_mod.c   |   2 +-
 drivers/gpu/drm/xe/xe_debug_metadata.h        | 100 +---
 drivers/gpu/drm/xe/xe_debug_metadata_types.h  |  29 +-
 drivers/gpu/drm/xe/xe_device.h                |   4 +-
 drivers/gpu/drm/xe/xe_device_types.h          |   6 +-
 drivers/gpu/drm/xe/xe_eudebug.h               | 106 +----
 drivers/gpu/drm/xe/xe_eudebug_types.h         | 450 +-----------------
 drivers/gpu/drm/xe/xe_exec_queue.c            |   4 +-
 drivers/gpu/drm/xe/xe_gt_debug.h              |  44 +-
 drivers/gpu/drm/xe/xe_sync_types.h            |   2 +-
 drivers/gpu/drm/xe/xe_vm.c                    |   4 +-
 drivers/gpu/drm/xe/xe_vm_types.h              |   4 +-
 include/uapi/drm/xe_drm.h                     |  92 ----
 include/uapi/drm/xe_drm_eudebug.h             | 256 ----------
 include/uapi/drm/xe_drm_prelim.h              | 412 ++++++++++++++++
 25 files changed, 1168 insertions(+), 1075 deletions(-)
 rename drivers/gpu/drm/xe/{ => prelim}/xe_debug_metadata.c (100%)
 create mode 100644 drivers/gpu/drm/xe/prelim/xe_debug_metadata.h
 create mode 100644 drivers/gpu/drm/xe/prelim/xe_debug_metadata_types.h
 rename drivers/gpu/drm/xe/{ => prelim}/xe_eudebug.c (100%)
 create mode 100644 drivers/gpu/drm/xe/prelim/xe_eudebug.h
 create mode 100644 drivers/gpu/drm/xe/prelim/xe_eudebug_types.h
 rename drivers/gpu/drm/xe/{ => prelim}/xe_gt_debug.c (100%)
 create mode 100644 drivers/gpu/drm/xe/prelim/xe_gt_debug.h
 delete mode 100644 include/uapi/drm/xe_drm_eudebug.h

diff --git a/drivers/gpu/drm/xe/Kconfig b/drivers/gpu/drm/xe/Kconfig
index 23f34e8e3151..fc3cab510405 100644
--- a/drivers/gpu/drm/xe/Kconfig
+++ b/drivers/gpu/drm/xe/Kconfig
@@ -85,7 +85,7 @@ config DRM_XE_FORCE_PROBE
 
 	  Use "!*" to block the probe of the driver for all known devices.
 
-config DRM_XE_EUDEBUG
+config PRELIM_DRM_XE_EUDEBUG
 	bool "Enable gdb debugger support (eudebug)"
 	depends on DRM_XE
 	default y
diff --git a/drivers/gpu/drm/xe/Makefile b/drivers/gpu/drm/xe/Makefile
index 317607e1e1fe..5ff0d4c2dc38 100644
--- a/drivers/gpu/drm/xe/Makefile
+++ b/drivers/gpu/drm/xe/Makefile
@@ -49,7 +49,7 @@ xe-y += xe_bb.o \
 	xe_gt_debugfs.o \
 	xe_gt_freq.o \
 	xe_gt_idle.o \
-	xe_gt_debug.o \
+	prelim/xe_gt_debug.o \
 	xe_gt_mcr.o \
 	xe_gt_pagefault.o \
 	xe_gt_sysfs.o \
@@ -117,8 +117,8 @@ xe-y += xe_bb.o \
 	xe_wa.o \
 	xe_wopcm.o
 
-xe-$(CONFIG_DRM_XE_EUDEBUG) += xe_eudebug.o \
-	xe_debug_metadata.o
+xe-$(CONFIG_PRELIM_DRM_XE_EUDEBUG) += prelim/xe_eudebug.o \
+	prelim/xe_debug_metadata.o
 
 xe-$(CONFIG_HMM_MIRROR) += xe_hmm.o
 
diff --git a/drivers/gpu/drm/xe/xe_debug_metadata.c b/drivers/gpu/drm/xe/prelim/xe_debug_metadata.c
similarity index 100%
rename from drivers/gpu/drm/xe/xe_debug_metadata.c
rename to drivers/gpu/drm/xe/prelim/xe_debug_metadata.c
diff --git a/drivers/gpu/drm/xe/prelim/xe_debug_metadata.h b/drivers/gpu/drm/xe/prelim/xe_debug_metadata.h
new file mode 100644
index 000000000000..57bc15e8f3f5
--- /dev/null
+++ b/drivers/gpu/drm/xe/prelim/xe_debug_metadata.h
@@ -0,0 +1,98 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#ifndef _XE_DEBUG_METADATA_H_
+#define _XE_DEBUG_METADATA_H_
+
+#include <linux/types.h>
+
+struct drm_device;
+struct drm_file;
+struct xe_file;
+
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
+
+#include "xe_debug_metadata_types.h"
+#include "xe_vm_types.h"
+
+struct xe_debug_metadata *xe_debug_metadata_get(struct xe_file *xef, u32 id);
+void xe_debug_metadata_put(struct xe_debug_metadata *mdata);
+
+int xe_debug_metadata_create_ioctl(struct drm_device *dev,
+				   void *data,
+				   struct drm_file *file);
+
+int xe_debug_metadata_destroy_ioctl(struct drm_device *dev,
+				    void *data,
+				    struct drm_file *file);
+
+static inline void xe_eudebug_move_vma_metadata(struct xe_eudebug_vma_metadata *from,
+						struct xe_eudebug_vma_metadata *to)
+{
+	list_splice_tail_init(&from->list, &to->list);
+}
+
+int xe_eudebug_copy_vma_metadata(struct xe_eudebug_vma_metadata *from,
+				 struct xe_eudebug_vma_metadata *to);
+void xe_eudebug_free_vma_metadata(struct xe_eudebug_vma_metadata *mdata);
+
+int vm_bind_op_ext_attach_debug(struct xe_device *xe,
+				struct xe_file *xef,
+				struct drm_gpuva_ops *ops,
+				u32 operation, u64 extension);
+
+#else /* CONFIG_DRM_XE_EUDEBUG */
+
+#include <linux/errno.h>
+
+struct xe_debug_metadata;
+struct xe_device;
+struct xe_eudebug_vma_metadata;
+struct drm_gpuva_ops;
+
+static inline struct xe_debug_metadata *xe_debug_metadata_get(struct xe_file *xef, u32 id) { return NULL; }
+static inline void xe_debug_metadata_put(struct xe_debug_metadata *mdata) { }
+
+static inline int xe_debug_metadata_create_ioctl(struct drm_device *dev,
+						 void *data,
+						 struct drm_file *file)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline int xe_debug_metadata_destroy_ioctl(struct drm_device *dev,
+						  void *data,
+						  struct drm_file *file)
+{
+	return -EOPNOTSUPP;
+}
+
+static inline void xe_eudebug_move_vma_metadata(struct xe_eudebug_vma_metadata *from,
+						struct xe_eudebug_vma_metadata *to)
+{
+}
+
+static inline int xe_eudebug_copy_vma_metadata(struct xe_eudebug_vma_metadata *from,
+					       struct xe_eudebug_vma_metadata *to)
+{
+	return 0;
+}
+
+static inline void xe_eudebug_free_vma_metadata(struct xe_eudebug_vma_metadata *mdata)
+{
+}
+
+static inline int vm_bind_op_ext_attach_debug(struct xe_device *xe,
+					      struct xe_file *xef,
+					      struct drm_gpuva_ops *ops,
+					      u32 operation, u64 extension)
+{
+	return -EINVAL;
+}
+
+#endif /* CONFIG_DRM_XE_EUDEBUG */
+
+
+#endif
diff --git a/drivers/gpu/drm/xe/prelim/xe_debug_metadata_types.h b/drivers/gpu/drm/xe/prelim/xe_debug_metadata_types.h
new file mode 100644
index 000000000000..624852920f58
--- /dev/null
+++ b/drivers/gpu/drm/xe/prelim/xe_debug_metadata_types.h
@@ -0,0 +1,25 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#ifndef _XE_DEBUG_METADATA_TYPES_H_
+#define _XE_DEBUG_METADATA_TYPES_H_
+
+#include <linux/kref.h>
+
+struct xe_debug_metadata {
+	/** @type: type of given metadata */
+	u64 type;
+
+	/** @ptr: copy of userptr, given as a metadata payload */
+	void *ptr;
+
+	/** @len: length, in bytes of the metadata */
+	u64 len;
+
+	/** @ref: reference count */
+	struct kref refcount;
+};
+
+#endif
diff --git a/drivers/gpu/drm/xe/xe_eudebug.c b/drivers/gpu/drm/xe/prelim/xe_eudebug.c
similarity index 100%
rename from drivers/gpu/drm/xe/xe_eudebug.c
rename to drivers/gpu/drm/xe/prelim/xe_eudebug.c
diff --git a/drivers/gpu/drm/xe/prelim/xe_eudebug.h b/drivers/gpu/drm/xe/prelim/xe_eudebug.h
new file mode 100644
index 000000000000..6644770bc2aa
--- /dev/null
+++ b/drivers/gpu/drm/xe/prelim/xe_eudebug.h
@@ -0,0 +1,106 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#ifndef _XE_EUDEBUG_H_
+
+#include <linux/types.h>
+
+struct drm_device;
+struct drm_file;
+struct xe_device;
+struct xe_file;
+struct xe_gt;
+struct xe_vm;
+struct xe_vma;
+struct xe_exec_queue;
+struct xe_hw_engine;
+struct xe_user_fence;
+struct xe_debug_metadata;
+struct drm_gpuva_ops;
+struct xe_eudebug_pagefault;
+
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
+
+int xe_eudebug_connect_ioctl(struct drm_device *dev,
+			     void *data,
+			     struct drm_file *file);
+
+void xe_eudebug_init(struct xe_device *xe);
+void xe_eudebug_fini(struct xe_device *xe);
+
+void xe_eudebug_file_open(struct xe_file *xef);
+void xe_eudebug_file_close(struct xe_file *xef);
+
+void xe_eudebug_vm_create(struct xe_file *xef, struct xe_vm *vm);
+void xe_eudebug_vm_destroy(struct xe_file *xef, struct xe_vm *vm);
+
+void xe_eudebug_exec_queue_create(struct xe_file *xef, struct xe_exec_queue *q);
+void xe_eudebug_exec_queue_destroy(struct xe_file *xef, struct xe_exec_queue *q);
+
+void xe_eudebug_vm_init(struct xe_vm *vm);
+void xe_eudebug_vm_bind_start(struct xe_vm *vm);
+void xe_eudebug_vm_bind_op_add(struct xe_vm *vm, u32 op, u64 addr, u64 range,
+			       struct drm_gpuva_ops *ops);
+void xe_eudebug_vm_bind_end(struct xe_vm *vm, bool has_ufence, int err);
+
+int xe_eudebug_vm_bind_ufence(struct xe_user_fence *ufence);
+void xe_eudebug_ufence_init(struct xe_user_fence *ufence, struct xe_file *xef, struct xe_vm *vm);
+void xe_eudebug_ufence_fini(struct xe_user_fence *ufence);
+
+struct xe_eudebug *xe_eudebug_get(struct xe_file *xef);
+void xe_eudebug_put(struct xe_eudebug *d);
+
+void xe_eudebug_debug_metadata_create(struct xe_file *xef, struct xe_debug_metadata *m);
+void xe_eudebug_debug_metadata_destroy(struct xe_file *xef, struct xe_debug_metadata *m);
+
+struct xe_eudebug_pagefault *xe_eudebug_pagefault_create(struct xe_gt *gt, struct xe_vm *vm,
+							 u64 page_addr, u8 fault_type,
+							 u8 fault_level, u8 access_type);
+void xe_eudebug_pagefault_process(struct xe_gt *gt, struct xe_eudebug_pagefault *pf);
+void xe_eudebug_pagefault_destroy(struct xe_gt *gt, struct xe_vm *vm,
+				  struct xe_eudebug_pagefault *pf, bool send_event);
+
+#else
+
+static inline int xe_eudebug_connect_ioctl(struct drm_device *dev,
+					   void *data,
+					   struct drm_file *file) { return 0; }
+
+static inline void xe_eudebug_init(struct xe_device *xe) { }
+static inline void xe_eudebug_fini(struct xe_device *xe) { }
+
+static inline void xe_eudebug_file_open(struct xe_file *xef) { }
+static inline void xe_eudebug_file_close(struct xe_file *xef) { }
+
+static inline void xe_eudebug_vm_create(struct xe_file *xef, struct xe_vm *vm) { }
+static inline void xe_eudebug_vm_destroy(struct xe_file *xef, struct xe_vm *vm) { }
+
+static inline void xe_eudebug_exec_queue_create(struct xe_file *xef, struct xe_exec_queue *q) { }
+static inline void xe_eudebug_exec_queue_destroy(struct xe_file *xef, struct xe_exec_queue *q) { }
+
+static inline void xe_eudebug_vm_init(struct xe_vm *vm) { }
+static inline void xe_eudebug_vm_bind_start(struct xe_vm *vm) { }
+static inline void xe_eudebug_vm_bind_op_add(struct xe_vm *vm, u32 op, u64 addr, u64 range, struct drm_gpuva_ops *ops) { }
+static inline void xe_eudebug_vm_bind_end(struct xe_vm *vm, bool has_ufence, int err) { }
+
+static inline int xe_eudebug_vm_bind_ufence(struct xe_user_fence *ufence) { return 0; }
+static inline void xe_eudebug_ufence_init(struct xe_user_fence *ufence, struct xe_file *xef, struct xe_vm *vm) { }
+static inline void xe_eudebug_ufence_fini(struct xe_user_fence *ufence) { }
+
+static inline struct xe_eudebug *xe_eudebug_get(struct xe_file *xef) { return NULL; }
+static inline void xe_eudebug_put(struct xe_eudebug *d) { }
+
+static inline void xe_eudebug_debug_metadata_create(struct xe_file *xef, struct xe_debug_metadata *m) { }
+static inline void xe_eudebug_debug_metadata_destroy(struct xe_file *xef, struct xe_debug_metadata *m) { }
+
+static inline struct xe_eudebug_pagefault *xe_eudebug_pagefault_create(struct xe_gt *gt, struct xe_vm *vm,
+								       u64 page_addr, u8 fault_type,
+								       u8 fault_level, u8 access_type) { return NULL; }
+static inline void xe_eudebug_pagefault_process(struct xe_gt *gt, struct xe_eudebug_pagefault *pf) { }
+static inline void xe_eudebug_pagefault_destroy(struct xe_gt *gt, struct xe_vm *vm, struct xe_eudebug_pagefault *pf, bool send_event) { }
+
+#endif /* CONFIG_PRELIM_DRM_XE_EUDEBUG */
+
+#endif
diff --git a/drivers/gpu/drm/xe/prelim/xe_eudebug_types.h b/drivers/gpu/drm/xe/prelim/xe_eudebug_types.h
new file mode 100644
index 000000000000..e3a028973469
--- /dev/null
+++ b/drivers/gpu/drm/xe/prelim/xe_eudebug_types.h
@@ -0,0 +1,449 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#ifndef __XE_EUDEBUG_TYPES_H_
+#define __XE_EUDEBUG_TYPES_H_
+
+#include <linux/completion.h>
+#include <linux/kfifo.h>
+#include <linux/kref.h>
+#include <linux/mutex.h>
+#include <linux/rbtree.h>
+#include <linux/rhashtable.h>
+#include <linux/wait.h>
+#include <linux/xarray.h>
+
+#include <uapi/drm/xe_drm.h>
+
+#include "xe_gt_debug.h"
+
+struct xe_device;
+struct task_struct;
+struct xe_eudebug;
+struct xe_eudebug_event;
+struct xe_hw_engine;
+struct workqueue_struct;
+struct xe_exec_queue;
+struct xe_lrc;
+
+#define CONFIG_DRM_XE_DEBUGGER_EVENT_QUEUE_SIZE 64
+
+/**
+ * struct xe_eudebug_handle - eudebug resource handle
+ */
+struct xe_eudebug_handle {
+	/** @key: key value in rhashtable <key:id> */
+	u64 key;
+
+	/** @id: opaque handle id for xarray <id:key> */
+	int id;
+
+	/** @rh_head: rhashtable head */
+	struct rhash_head rh_head;
+};
+
+/**
+ * struct xe_eudebug_resource - Resource map for one resource
+ */
+struct xe_eudebug_resource {
+	/** @xa: xarrays for <id->key> */
+	struct xarray xa;
+
+	/** @rh rhashtable for <key->id> */
+	struct rhashtable rh;
+};
+
+#define XE_EUDEBUG_RES_TYPE_CLIENT	0
+#define XE_EUDEBUG_RES_TYPE_VM		1
+#define XE_EUDEBUG_RES_TYPE_EXEC_QUEUE	2
+#define XE_EUDEBUG_RES_TYPE_LRC		3
+#define XE_EUDEBUG_RES_TYPE_METADATA	4
+#define XE_EUDEBUG_RES_TYPE_COUNT	(XE_EUDEBUG_RES_TYPE_METADATA + 1)
+
+/**
+ * struct xe_eudebug_resources - eudebug resources for all types
+ */
+struct xe_eudebug_resources {
+	/** @lock: guards access into rt */
+	struct mutex lock;
+
+	/** @rt: resource maps for all types */
+	struct xe_eudebug_resource rt[XE_EUDEBUG_RES_TYPE_COUNT];
+};
+
+/**
+ * struct xe_eudebug_eu_control_ops - interface for eu thread
+ * state control backend
+ */
+struct xe_eudebug_eu_control_ops {
+	/** @interrupt_all: interrupts workload active on given hwe */
+	int (*interrupt_all)(struct xe_eudebug *e, struct xe_exec_queue *q,
+			     struct xe_lrc *lrc);
+
+	/** @resume: resumes threads reflected by bitmask active on given hwe */
+	int (*resume)(struct xe_eudebug *e, struct xe_exec_queue *q,
+		      struct xe_lrc *lrc, u8 *bitmap, unsigned int bitmap_size);
+
+	/** @stopped: returns bitmap reflecting threads which signal attention */
+	int (*stopped)(struct xe_eudebug *e, struct xe_exec_queue *q,
+		       struct xe_lrc *lrc, u8 *bitmap, unsigned int bitmap_size);
+};
+
+/**
+ * struct xe_eudebug - Top level struct for eudebug: the connection
+ */
+struct xe_eudebug {
+	/** @ref: kref counter for this struct */
+	struct kref ref;
+
+	/** @rcu: rcu_head for rcu destruction */
+	struct rcu_head rcu;
+
+	/** @connection_link: our link into the xe_device:eudebug.list */
+	struct list_head connection_link;
+
+	struct {
+		/** @status: connected = 1, disconnected = error */
+#define XE_EUDEBUG_STATUS_CONNECTED 1
+		int status;
+
+		/** @lock: guards access to status */
+		spinlock_t lock;
+	} connection;
+
+	/** @xe: the parent device we are serving */
+	struct xe_device *xe;
+
+	/** @target_task: the task that we are debugging */
+	struct task_struct *target_task;
+
+	/** @res: the resource maps we track for target_task */
+	struct xe_eudebug_resources *res;
+
+	/** @session: session number for this connection (for logs) */
+	u64 session;
+
+	/** @discovery: completion to wait for discovery */
+	struct completion discovery;
+
+	/** @discovery_work: worker to discover resources for target_task */
+	struct work_struct discovery_work;
+
+	/** eu_lock: guards operations on eus (eu thread control and attention) */
+	struct mutex eu_lock;
+
+	/** @events: kfifo queue of to-be-delivered events */
+	struct {
+		/** @lock: guards access to fifo */
+		spinlock_t lock;
+
+		/** @fifo: queue of events pending */
+		DECLARE_KFIFO(fifo,
+			      struct xe_eudebug_event *,
+			      CONFIG_DRM_XE_DEBUGGER_EVENT_QUEUE_SIZE);
+
+		/** @write_done: waitqueue for signalling write to fifo */
+		wait_queue_head_t write_done;
+
+		/** @read_done: waitqueue for signalling read from fifo */
+		wait_queue_head_t read_done;
+
+		/** @event_seqno: seqno counter to stamp events for fifo */
+		atomic_long_t seqno;
+	} events;
+
+	/* user fences tracked by this debugger */
+	struct {
+		/** @lock: guards access to tree */
+		spinlock_t lock;
+
+		struct rb_root tree;
+	} acks;
+
+	/** @ops operations for eu_control */
+	struct xe_eudebug_eu_control_ops *ops;
+
+	/** @pf_lock: guards access to pagefaults list*/
+	struct mutex pf_lock;
+	/** @pagefaults: xe_eudebug_pagefault list for pagefault event queuing */
+	struct list_head pagefaults;
+	/**
+	 * @pf_fence: fence on operations of eus (eu thread control and attention)
+	 * when page faults are being handled, protected by @eu_lock.
+	 */
+	struct dma_fence __rcu *pf_fence;
+};
+
+/**
+ * struct xe_eudebug_event - Internal base event struct for eudebug
+ */
+struct xe_eudebug_event {
+	/** @len: length of this event, including payload */
+	u32 len;
+
+	/** @type: message type */
+	u16 type;
+
+	/** @flags: message flags */
+	u16 flags;
+
+	/** @seqno: sequence number for ordering */
+	u64 seqno;
+
+	/** @reserved: reserved field MBZ */
+	u64 reserved;
+
+	/** @data: payload bytes */
+	u8 data[];
+};
+
+struct xe_eudebug_event_envelope {
+	struct list_head link;
+	struct xe_eudebug_event *event;
+};
+
+/**
+ * struct xe_eudebug_event_open - Internal event for client open/close
+ */
+struct xe_eudebug_event_open {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: opaque handle for client */
+	u64 client_handle;
+};
+
+/**
+ * struct xe_eudebug_event_vm - Internal event for vm open/close
+ */
+struct xe_eudebug_event_vm {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client containing the vm open/close */
+	u64 client_handle;
+
+	/** @vm_handle: vm handle it's open/close */
+	u64 vm_handle;
+};
+
+/**
+ * struct xe_eudebug_event_exec_queue - Internal event for
+ * exec_queue create/destroy
+ */
+struct xe_eudebug_event_exec_queue {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client for the engine create/destroy */
+	u64 client_handle;
+
+	/** @vm_handle: vm handle for the engine create/destroy */
+	u64 vm_handle;
+
+	/** @exec_queue_handle: engine handle */
+	u64 exec_queue_handle;
+
+	/** @engine_handle: engine class */
+	u32 engine_class;
+
+	/** @width: submission width (number BB per exec) for this exec queue */
+	u32 width;
+
+	/** @lrc_handles: handles for each logical ring context created with this exec queue */
+	u64 lrc_handle[] __counted_by(width);
+};
+
+struct xe_eudebug_event_exec_queue_placements {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client for the engine create/destroy */
+	u64 client_handle;
+
+	/** @vm_handle: vm handle for the engine create/destroy */
+	u64 vm_handle;
+
+	/** @exec_queue_handle: engine handle */
+	u64 exec_queue_handle;
+
+	/** @engine_handle: engine class */
+	u64 lrc_handle;
+
+	/** @num_placements: all possible placements for given lrc */
+	u32 num_placements;
+
+	/** @pad: padding */
+	u32 pad;
+
+	/** @instances: num_placements sized array containing drm_xe_engine_class_instance*/
+	u64 instances[]; __counted_by(num_placements);
+};
+
+/**
+ * struct xe_eudebug_event_eu_attention - Internal event for EU attention
+ */
+struct xe_eudebug_event_eu_attention {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client for the attention */
+	u64 client_handle;
+
+	/** @exec_queue_handle: handle of exec_queue which raised attention */
+	u64 exec_queue_handle;
+
+	/** @lrc_handle: lrc handle of the workload which raised attention */
+	u64 lrc_handle;
+
+	/** @flags: eu attention event flags, currently MBZ */
+	u32 flags;
+
+	/** @bitmask_size: size of the bitmask, specific to device */
+	u32 bitmask_size;
+
+	/**
+	 * @bitmask: reflects threads currently signalling attention,
+	 * starting from natural hardware order of DSS=0, eu=0
+	 */
+	u8 bitmask[] __counted_by(bitmask_size);
+};
+
+/**
+ * struct xe_eudebug_event_vm_bind - Internal event for vm bind/unbind operation
+ */
+struct xe_eudebug_event_vm_bind {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	u64 client_handle;
+	u64 vm_handle;
+
+	u32 flags;
+	u32 num_binds;
+};
+
+struct xe_eudebug_event_vm_bind_op {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+	u64 vm_bind_ref_seqno;
+	u64 num_extensions;
+
+	u64 addr; /* Zero for unmap all ? */
+	u64 range; /* Zero for unmap all ? */
+};
+
+struct xe_eudebug_event_vm_bind_ufence {
+	struct xe_eudebug_event base;
+	u64 vm_bind_ref_seqno;
+};
+
+struct xe_eudebug_event_metadata {
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client for the attention */
+	u64 client_handle;
+
+	/** @metadata_handle: debug metadata handle it's created/destroyed */
+	u64 metadata_handle;
+
+	/* @type: metadata type, refer to xe_drm.h for options */
+	u64 type;
+
+	/* @len: size of metadata paylad */
+	u64 len;
+};
+
+struct xe_eudebug_event_vm_bind_op_metadata {
+	struct xe_eudebug_event base;
+	u64 vm_bind_op_ref_seqno;
+
+	u64 metadata_handle;
+	u64 metadata_cookie;
+};
+
+/**
+ * struct xe_eudebug_event_pagefault - Internal event for EU Pagefault
+ */
+struct xe_eudebug_event_pagefault {
+	/** @base: base event */
+	struct xe_eudebug_event base;
+
+	/** @client_handle: client for the Pagefault */
+	u64 client_handle;
+
+	/** @exec_queue_handle: handle of exec_queue which raised Pagefault */
+	u64 exec_queue_handle;
+
+	/** @lrc_handle: lrc handle of the workload which raised Pagefault */
+	u64 lrc_handle;
+
+	/** @flags: eu Pagefault event flags, currently MBZ */
+	u32 flags;
+
+	/**
+	 * @bitmask_size: sum of size before/after/resolved att bits.
+	 * It has three times the size of xe_eudebug_event_eu_attention.bitmask_size.
+	 */
+	u32 bitmask_size;
+
+	/** @pagefault_address: The ppgtt address where the Pagefault occurred */
+	u64 pagefault_address;
+
+	/**
+	 * @bitmask: Bitmask of thread attentions starting from natural,
+	 * hardware order of DSS=0, eu=0, 8 attention bits per eu.
+	 * The order of the bitmask array is before, after, resolved.
+	 */
+	u8 bitmask[];
+};
+
+/**
+ * struct xe_eudebug_pagefault - eudebug structure for queuing pagefault
+ */
+struct xe_eudebug_pagefault {
+	/** @list: link into the xe_eudebug.pagefaults */
+	struct list_head list;
+	/** @q: exec_queue which raised pagefault */
+	struct xe_exec_queue *q;
+	/** @lrc_idx: lrc index of the workload which raised pagefault */
+	int lrc_idx;
+
+	/* pagefault raw partial data passed from guc*/
+	struct {
+		/** @addr: ppgtt address where the pagefault occurred */
+		u64 addr;
+		int type;
+		int level;
+		int access;
+	} fault;
+
+	struct {
+		/** @before: state of attention bits before page fault WA processing*/
+		struct xe_eu_attentions before;
+		/**
+		 * @after: status of attention bits during page fault WA processing.
+		 * It includes eu threads where attention bits are turned on for
+		 * reasons other than page fault WA (breakpoint, interrupt, etc.).
+		 */
+		struct xe_eu_attentions after;
+		/**
+		 * @resolved: state of the attention bits after page fault WA.
+		 * It includes the eu thread that caused the page fault.
+		 * To determine the eu thread that caused the page fault,
+		 * do XOR attentions.after and attentions.resolved.
+		 */
+		struct xe_eu_attentions resolved;
+	} attentions;
+
+	/**
+	 * @deferred_resolved: to update attentions.resolved again when attention
+	 * bits are ready if the eu thread fails to turn on attention bits within
+	 * a certain time after page fault WA processing.
+	 */
+	bool deferred_resolved;
+};
+
+#endif
diff --git a/drivers/gpu/drm/xe/xe_gt_debug.c b/drivers/gpu/drm/xe/prelim/xe_gt_debug.c
similarity index 100%
rename from drivers/gpu/drm/xe/xe_gt_debug.c
rename to drivers/gpu/drm/xe/prelim/xe_gt_debug.c
diff --git a/drivers/gpu/drm/xe/prelim/xe_gt_debug.h b/drivers/gpu/drm/xe/prelim/xe_gt_debug.h
new file mode 100644
index 000000000000..3e3fb501eedd
--- /dev/null
+++ b/drivers/gpu/drm/xe/prelim/xe_gt_debug.h
@@ -0,0 +1,42 @@
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#ifndef __XE_GT_DEBUG_
+#define __XE_GT_DEBUG_
+
+#define TD_EU_ATTENTION_MAX_ROWS 2u
+
+#include "xe_gt_types.h"
+
+#define XE_GT_ATTENTION_TIMEOUT_MS 100
+
+struct xe_eu_attentions {
+#define XE_MAX_EUS 1024
+#define XE_MAX_THREADS 10
+
+	u8 att[DIV_ROUND_UP(XE_MAX_EUS * XE_MAX_THREADS, BITS_PER_BYTE)];
+	unsigned int size;
+	ktime_t ts;
+};
+
+int xe_gt_eu_threads_needing_attention(struct xe_gt *gt);
+int xe_gt_foreach_dss_group_instance(struct xe_gt *gt,
+				     int (*fn)(struct xe_gt *gt,
+					       void *data,
+					       u16 group,
+					       u16 instance),
+				     void *data);
+
+int xe_gt_eu_attention_bitmap_size(struct xe_gt *gt);
+int xe_gt_eu_attention_bitmap(struct xe_gt *gt, u8 *bits,
+			      unsigned int bitmap_size);
+
+void xe_gt_eu_attentions_read(struct xe_gt *gt,
+			      struct xe_eu_attentions *a,
+			      const unsigned int settle_time_ms);
+
+unsigned int xe_eu_attentions_xor_count(const struct xe_eu_attentions *a,
+					const struct xe_eu_attentions *b);
+#endif
diff --git a/drivers/gpu/drm/xe/tests/xe_live_test_mod.c b/drivers/gpu/drm/xe/tests/xe_live_test_mod.c
index e95b1c6f5695..26c58eea7a4c 100644
--- a/drivers/gpu/drm/xe/tests/xe_live_test_mod.c
+++ b/drivers/gpu/drm/xe/tests/xe_live_test_mod.c
@@ -4,7 +4,7 @@
  */
 #include <linux/module.h>
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 extern struct kunit_suite xe_eudebug_test_suite;
 kunit_test_suite(xe_eudebug_test_suite);
 #endif
diff --git a/drivers/gpu/drm/xe/xe_debug_metadata.h b/drivers/gpu/drm/xe/xe_debug_metadata.h
index 13b763ee06e1..a8db29da8ac5 100644
--- a/drivers/gpu/drm/xe/xe_debug_metadata.h
+++ b/drivers/gpu/drm/xe/xe_debug_metadata.h
@@ -1,98 +1,6 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
+#ifndef __WRAP_XE_DEBUG_METADATA_H
+#define	__WRAP_XE_DEBUG_METADATA_H
 
-#ifndef _XE_DEBUG_METADATA_H_
-#define _XE_DEBUG_METADATA_H_
+#include "prelim/xe_debug_metadata.h"
 
-#include <linux/types.h>
-
-struct drm_device;
-struct drm_file;
-struct xe_file;
-
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
-
-#include "xe_debug_metadata_types.h"
-#include "xe_vm_types.h"
-
-struct xe_debug_metadata *xe_debug_metadata_get(struct xe_file *xef, u32 id);
-void xe_debug_metadata_put(struct xe_debug_metadata *mdata);
-
-int xe_debug_metadata_create_ioctl(struct drm_device *dev,
-				   void *data,
-				   struct drm_file *file);
-
-int xe_debug_metadata_destroy_ioctl(struct drm_device *dev,
-				    void *data,
-				    struct drm_file *file);
-
-static inline void xe_eudebug_move_vma_metadata(struct xe_eudebug_vma_metadata *from,
-						struct xe_eudebug_vma_metadata *to)
-{
-	list_splice_tail_init(&from->list, &to->list);
-}
-
-int xe_eudebug_copy_vma_metadata(struct xe_eudebug_vma_metadata *from,
-				 struct xe_eudebug_vma_metadata *to);
-void xe_eudebug_free_vma_metadata(struct xe_eudebug_vma_metadata *mdata);
-
-int vm_bind_op_ext_attach_debug(struct xe_device *xe,
-				struct xe_file *xef,
-				struct drm_gpuva_ops *ops,
-				u32 operation, u64 extension);
-
-#else /* CONFIG_DRM_XE_EUDEBUG */
-
-#include <linux/errno.h>
-
-struct xe_debug_metadata;
-struct xe_device;
-struct xe_eudebug_vma_metadata;
-struct drm_gpuva_ops;
-
-static inline struct xe_debug_metadata *xe_debug_metadata_get(struct xe_file *xef, u32 id) { return NULL; }
-static inline void xe_debug_metadata_put(struct xe_debug_metadata *mdata) { }
-
-static inline int xe_debug_metadata_create_ioctl(struct drm_device *dev,
-						 void *data,
-						 struct drm_file *file)
-{
-	return -EOPNOTSUPP;
-}
-
-static inline int xe_debug_metadata_destroy_ioctl(struct drm_device *dev,
-						  void *data,
-						  struct drm_file *file)
-{
-	return -EOPNOTSUPP;
-}
-
-static inline void xe_eudebug_move_vma_metadata(struct xe_eudebug_vma_metadata *from,
-						struct xe_eudebug_vma_metadata *to)
-{
-}
-
-static inline int xe_eudebug_copy_vma_metadata(struct xe_eudebug_vma_metadata *from,
-					       struct xe_eudebug_vma_metadata *to)
-{
-	return 0;
-}
-
-static inline void xe_eudebug_free_vma_metadata(struct xe_eudebug_vma_metadata *mdata)
-{
-}
-
-static inline int vm_bind_op_ext_attach_debug(struct xe_device *xe,
-					      struct xe_file *xef,
-					      struct drm_gpuva_ops *ops,
-					      u32 operation, u64 extension)
-{
-	return -EINVAL;
-}
-
-#endif /* CONFIG_DRM_XE_EUDEBUG */
-
-
-#endif
+#endif /* __WRAP_XE_DEBUG_METADATA_H */
diff --git a/drivers/gpu/drm/xe/xe_debug_metadata_types.h b/drivers/gpu/drm/xe/xe_debug_metadata_types.h
index 624852920f58..e1dca4556785 100644
--- a/drivers/gpu/drm/xe/xe_debug_metadata_types.h
+++ b/drivers/gpu/drm/xe/xe_debug_metadata_types.h
@@ -1,25 +1,4 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
-
-#ifndef _XE_DEBUG_METADATA_TYPES_H_
-#define _XE_DEBUG_METADATA_TYPES_H_
-
-#include <linux/kref.h>
-
-struct xe_debug_metadata {
-	/** @type: type of given metadata */
-	u64 type;
-
-	/** @ptr: copy of userptr, given as a metadata payload */
-	void *ptr;
-
-	/** @len: length, in bytes of the metadata */
-	u64 len;
-
-	/** @ref: reference count */
-	struct kref refcount;
-};
-
-#endif
+#ifndef _WRAP_XE_DEBUG_METADATA_TYPES_H_
+#define _WRAP_XE_DEBUG_METADATA_TYPES_H_
+#include "prelim/xe_debug_metadata_types.h"
+#endif /* _WRAP_XE_DEBUG_METADATA_TYPES_H_ */
diff --git a/drivers/gpu/drm/xe/xe_device.h b/drivers/gpu/drm/xe/xe_device.h
index b54ad38686ce..498255cc789e 100644
--- a/drivers/gpu/drm/xe/xe_device.h
+++ b/drivers/gpu/drm/xe/xe_device.h
@@ -189,7 +189,7 @@ void xe_file_put(struct xe_file *xef);
 #define LNL_FLUSH_WORK(wrk__) \
 	flush_work(wrk__)
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 static inline int xe_eudebug_needs_lock(const unsigned int cmd)
 {
 	const unsigned int xe_cmd = DRM_IOCTL_NR(cmd) - DRM_COMMAND_BASE;
@@ -222,6 +222,6 @@ static inline void xe_eudebug_discovery_unlock(struct xe_device *xe, unsigned in
 #else
 static inline void xe_eudebug_discovery_lock(struct xe_device *xe, unsigned int cmd) { }
 static inline void xe_eudebug_discovery_unlock(struct xe_device *xe, unsigned int cmd) { }
-#endif /* CONFIG_DRM_XE_EUDEBUG */
+#endif /* CONFIG_PRELIM_DRM_XE_EUDEBUG */
 
 #endif
diff --git a/drivers/gpu/drm/xe/xe_device_types.h b/drivers/gpu/drm/xe/xe_device_types.h
index 1dde69a199af..6833ad6a9f0f 100644
--- a/drivers/gpu/drm/xe/xe_device_types.h
+++ b/drivers/gpu/drm/xe/xe_device_types.h
@@ -346,7 +346,7 @@ struct xe_device {
 		struct workqueue_struct *wq;
 	} sriov;
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	/** @clients: eudebug clients info */
 	struct {
 		/** @clients.lock: Protects client list */
@@ -497,7 +497,7 @@ struct xe_device {
 	u8 vm_inject_error_position;
 #endif
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	/** @debugger connection list and globals for device */
 	struct {
 		/** @lock: protects the list of connections */
@@ -627,7 +627,7 @@ struct xe_file {
 	/** @refcount: ref count of this xe file */
 	struct kref refcount;
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	struct {
 		/** @client_link: list entry in xe_device.clients.list */
 		struct list_head client_link;
diff --git a/drivers/gpu/drm/xe/xe_eudebug.h b/drivers/gpu/drm/xe/xe_eudebug.h
index 5f6f58ed70d4..24f4b02662ea 100644
--- a/drivers/gpu/drm/xe/xe_eudebug.h
+++ b/drivers/gpu/drm/xe/xe_eudebug.h
@@ -1,106 +1,6 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
+#ifndef _WRAP_XE_EUDEBUG_
+#define _WRAP_XE_EUDEBUG_
 
-#ifndef _XE_EUDEBUG_H_
-
-#include <linux/types.h>
-
-struct drm_device;
-struct drm_file;
-struct xe_device;
-struct xe_file;
-struct xe_gt;
-struct xe_vm;
-struct xe_vma;
-struct xe_exec_queue;
-struct xe_hw_engine;
-struct xe_user_fence;
-struct xe_debug_metadata;
-struct drm_gpuva_ops;
-struct xe_eudebug_pagefault;
-
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
-
-int xe_eudebug_connect_ioctl(struct drm_device *dev,
-			     void *data,
-			     struct drm_file *file);
-
-void xe_eudebug_init(struct xe_device *xe);
-void xe_eudebug_fini(struct xe_device *xe);
-
-void xe_eudebug_file_open(struct xe_file *xef);
-void xe_eudebug_file_close(struct xe_file *xef);
-
-void xe_eudebug_vm_create(struct xe_file *xef, struct xe_vm *vm);
-void xe_eudebug_vm_destroy(struct xe_file *xef, struct xe_vm *vm);
-
-void xe_eudebug_exec_queue_create(struct xe_file *xef, struct xe_exec_queue *q);
-void xe_eudebug_exec_queue_destroy(struct xe_file *xef, struct xe_exec_queue *q);
-
-void xe_eudebug_vm_init(struct xe_vm *vm);
-void xe_eudebug_vm_bind_start(struct xe_vm *vm);
-void xe_eudebug_vm_bind_op_add(struct xe_vm *vm, u32 op, u64 addr, u64 range,
-			       struct drm_gpuva_ops *ops);
-void xe_eudebug_vm_bind_end(struct xe_vm *vm, bool has_ufence, int err);
-
-int xe_eudebug_vm_bind_ufence(struct xe_user_fence *ufence);
-void xe_eudebug_ufence_init(struct xe_user_fence *ufence, struct xe_file *xef, struct xe_vm *vm);
-void xe_eudebug_ufence_fini(struct xe_user_fence *ufence);
-
-struct xe_eudebug *xe_eudebug_get(struct xe_file *xef);
-void xe_eudebug_put(struct xe_eudebug *d);
-
-void xe_eudebug_debug_metadata_create(struct xe_file *xef, struct xe_debug_metadata *m);
-void xe_eudebug_debug_metadata_destroy(struct xe_file *xef, struct xe_debug_metadata *m);
-
-struct xe_eudebug_pagefault *xe_eudebug_pagefault_create(struct xe_gt *gt, struct xe_vm *vm,
-							 u64 page_addr, u8 fault_type,
-							 u8 fault_level, u8 access_type);
-void xe_eudebug_pagefault_process(struct xe_gt *gt, struct xe_eudebug_pagefault *pf);
-void xe_eudebug_pagefault_destroy(struct xe_gt *gt, struct xe_vm *vm,
-				  struct xe_eudebug_pagefault *pf, bool send_event);
-
-#else
-
-static inline int xe_eudebug_connect_ioctl(struct drm_device *dev,
-					   void *data,
-					   struct drm_file *file) { return 0; }
-
-static inline void xe_eudebug_init(struct xe_device *xe) { }
-static inline void xe_eudebug_fini(struct xe_device *xe) { }
-
-static inline void xe_eudebug_file_open(struct xe_file *xef) { }
-static inline void xe_eudebug_file_close(struct xe_file *xef) { }
-
-static inline void xe_eudebug_vm_create(struct xe_file *xef, struct xe_vm *vm) { }
-static inline void xe_eudebug_vm_destroy(struct xe_file *xef, struct xe_vm *vm) { }
-
-static inline void xe_eudebug_exec_queue_create(struct xe_file *xef, struct xe_exec_queue *q) { }
-static inline void xe_eudebug_exec_queue_destroy(struct xe_file *xef, struct xe_exec_queue *q) { }
-
-static inline void xe_eudebug_vm_init(struct xe_vm *vm) { }
-static inline void xe_eudebug_vm_bind_start(struct xe_vm *vm) { }
-static inline void xe_eudebug_vm_bind_op_add(struct xe_vm *vm, u32 op, u64 addr, u64 range, struct drm_gpuva_ops *ops) { }
-static inline void xe_eudebug_vm_bind_end(struct xe_vm *vm, bool has_ufence, int err) { }
-
-static inline int xe_eudebug_vm_bind_ufence(struct xe_user_fence *ufence) { return 0; }
-static inline void xe_eudebug_ufence_init(struct xe_user_fence *ufence, struct xe_file *xef, struct xe_vm *vm) { }
-static inline void xe_eudebug_ufence_fini(struct xe_user_fence *ufence) { }
-
-static inline struct xe_eudebug *xe_eudebug_get(struct xe_file *xef) { return NULL; }
-static inline void xe_eudebug_put(struct xe_eudebug *d) { }
-
-static inline void xe_eudebug_debug_metadata_create(struct xe_file *xef, struct xe_debug_metadata *m) { }
-static inline void xe_eudebug_debug_metadata_destroy(struct xe_file *xef, struct xe_debug_metadata *m) { }
-
-static inline struct xe_eudebug_pagefault *xe_eudebug_pagefault_create(struct xe_gt *gt, struct xe_vm *vm,
-								       u64 page_addr, u8 fault_type,
-								       u8 fault_level, u8 access_type) { return NULL; }
-static inline void xe_eudebug_pagefault_process(struct xe_gt *gt, struct xe_eudebug_pagefault *pf) { }
-static inline void xe_eudebug_pagefault_destroy(struct xe_gt *gt, struct xe_vm *vm, struct xe_eudebug_pagefault *pf, bool send_event) { }
-
-#endif /* CONFIG_DRM_XE_EUDEBUG */
+#include "prelim/xe_eudebug.h"
 
 #endif
diff --git a/drivers/gpu/drm/xe/xe_eudebug_types.h b/drivers/gpu/drm/xe/xe_eudebug_types.h
index 00853dacd477..7244eec6417b 100644
--- a/drivers/gpu/drm/xe/xe_eudebug_types.h
+++ b/drivers/gpu/drm/xe/xe_eudebug_types.h
@@ -1,448 +1,6 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
+#ifndef __WRAP_XE_EUDEBUG_TYPES_H_
+#define __WRAP_XE_EUDEBUG_TYPES_H_
 
-#ifndef __XE_EUDEBUG_TYPES_H_
+#include "prelim/xe_eudebug_types.h"
 
-#include <linux/completion.h>
-#include <linux/kfifo.h>
-#include <linux/kref.h>
-#include <linux/mutex.h>
-#include <linux/rbtree.h>
-#include <linux/rhashtable.h>
-#include <linux/wait.h>
-#include <linux/xarray.h>
-
-#include <uapi/drm/xe_drm.h>
-
-#include "xe_gt_debug.h"
-
-struct xe_device;
-struct task_struct;
-struct xe_eudebug;
-struct xe_eudebug_event;
-struct xe_hw_engine;
-struct workqueue_struct;
-struct xe_exec_queue;
-struct xe_lrc;
-
-#define CONFIG_DRM_XE_DEBUGGER_EVENT_QUEUE_SIZE 64
-
-/**
- * struct xe_eudebug_handle - eudebug resource handle
- */
-struct xe_eudebug_handle {
-	/** @key: key value in rhashtable <key:id> */
-	u64 key;
-
-	/** @id: opaque handle id for xarray <id:key> */
-	int id;
-
-	/** @rh_head: rhashtable head */
-	struct rhash_head rh_head;
-};
-
-/**
- * struct xe_eudebug_resource - Resource map for one resource
- */
-struct xe_eudebug_resource {
-	/** @xa: xarrays for <id->key> */
-	struct xarray xa;
-
-	/** @rh rhashtable for <key->id> */
-	struct rhashtable rh;
-};
-
-#define XE_EUDEBUG_RES_TYPE_CLIENT	0
-#define XE_EUDEBUG_RES_TYPE_VM		1
-#define XE_EUDEBUG_RES_TYPE_EXEC_QUEUE	2
-#define XE_EUDEBUG_RES_TYPE_LRC		3
-#define XE_EUDEBUG_RES_TYPE_METADATA	4
-#define XE_EUDEBUG_RES_TYPE_COUNT	(XE_EUDEBUG_RES_TYPE_METADATA + 1)
-
-/**
- * struct xe_eudebug_resources - eudebug resources for all types
- */
-struct xe_eudebug_resources {
-	/** @lock: guards access into rt */
-	struct mutex lock;
-
-	/** @rt: resource maps for all types */
-	struct xe_eudebug_resource rt[XE_EUDEBUG_RES_TYPE_COUNT];
-};
-
-/**
- * struct xe_eudebug_eu_control_ops - interface for eu thread
- * state control backend
- */
-struct xe_eudebug_eu_control_ops {
-	/** @interrupt_all: interrupts workload active on given hwe */
-	int (*interrupt_all)(struct xe_eudebug *e, struct xe_exec_queue *q,
-			     struct xe_lrc *lrc);
-
-	/** @resume: resumes threads reflected by bitmask active on given hwe */
-	int (*resume)(struct xe_eudebug *e, struct xe_exec_queue *q,
-		      struct xe_lrc *lrc, u8 *bitmap, unsigned int bitmap_size);
-
-	/** @stopped: returns bitmap reflecting threads which signal attention */
-	int (*stopped)(struct xe_eudebug *e, struct xe_exec_queue *q,
-		       struct xe_lrc *lrc, u8 *bitmap, unsigned int bitmap_size);
-};
-
-/**
- * struct xe_eudebug - Top level struct for eudebug: the connection
- */
-struct xe_eudebug {
-	/** @ref: kref counter for this struct */
-	struct kref ref;
-
-	/** @rcu: rcu_head for rcu destruction */
-	struct rcu_head rcu;
-
-	/** @connection_link: our link into the xe_device:eudebug.list */
-	struct list_head connection_link;
-
-	struct {
-		/** @status: connected = 1, disconnected = error */
-#define XE_EUDEBUG_STATUS_CONNECTED 1
-		int status;
-
-		/** @lock: guards access to status */
-		spinlock_t lock;
-	} connection;
-
-	/** @xe: the parent device we are serving */
-	struct xe_device *xe;
-
-	/** @target_task: the task that we are debugging */
-	struct task_struct *target_task;
-
-	/** @res: the resource maps we track for target_task */
-	struct xe_eudebug_resources *res;
-
-	/** @session: session number for this connection (for logs) */
-	u64 session;
-
-	/** @discovery: completion to wait for discovery */
-	struct completion discovery;
-
-	/** @discovery_work: worker to discover resources for target_task */
-	struct work_struct discovery_work;
-
-	/** eu_lock: guards operations on eus (eu thread control and attention) */
-	struct mutex eu_lock;
-
-	/** @events: kfifo queue of to-be-delivered events */
-	struct {
-		/** @lock: guards access to fifo */
-		spinlock_t lock;
-
-		/** @fifo: queue of events pending */
-		DECLARE_KFIFO(fifo,
-			      struct xe_eudebug_event *,
-			      CONFIG_DRM_XE_DEBUGGER_EVENT_QUEUE_SIZE);
-
-		/** @write_done: waitqueue for signalling write to fifo */
-		wait_queue_head_t write_done;
-
-		/** @read_done: waitqueue for signalling read from fifo */
-		wait_queue_head_t read_done;
-
-		/** @event_seqno: seqno counter to stamp events for fifo */
-		atomic_long_t seqno;
-	} events;
-
-	/* user fences tracked by this debugger */
-	struct {
-		/** @lock: guards access to tree */
-		spinlock_t lock;
-
-		struct rb_root tree;
-	} acks;
-
-	/** @ops operations for eu_control */
-	struct xe_eudebug_eu_control_ops *ops;
-
-	/** @pf_lock: guards access to pagefaults list*/
-	struct mutex pf_lock;
-	/** @pagefaults: xe_eudebug_pagefault list for pagefault event queuing */
-	struct list_head pagefaults;
-	/**
-	 * @pf_fence: fence on operations of eus (eu thread control and attention)
-	 * when page faults are being handled, protected by @eu_lock.
-	 */
-	struct dma_fence __rcu *pf_fence;
-};
-
-/**
- * struct xe_eudebug_event - Internal base event struct for eudebug
- */
-struct xe_eudebug_event {
-	/** @len: length of this event, including payload */
-	u32 len;
-
-	/** @type: message type */
-	u16 type;
-
-	/** @flags: message flags */
-	u16 flags;
-
-	/** @seqno: sequence number for ordering */
-	u64 seqno;
-
-	/** @reserved: reserved field MBZ */
-	u64 reserved;
-
-	/** @data: payload bytes */
-	u8 data[];
-};
-
-struct xe_eudebug_event_envelope {
-	struct list_head link;
-	struct xe_eudebug_event *event;
-};
-
-/**
- * struct xe_eudebug_event_open - Internal event for client open/close
- */
-struct xe_eudebug_event_open {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: opaque handle for client */
-	u64 client_handle;
-};
-
-/**
- * struct xe_eudebug_event_vm - Internal event for vm open/close
- */
-struct xe_eudebug_event_vm {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client containing the vm open/close */
-	u64 client_handle;
-
-	/** @vm_handle: vm handle it's open/close */
-	u64 vm_handle;
-};
-
-/**
- * struct xe_eudebug_event_exec_queue - Internal event for
- * exec_queue create/destroy
- */
-struct xe_eudebug_event_exec_queue {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client for the engine create/destroy */
-	u64 client_handle;
-
-	/** @vm_handle: vm handle for the engine create/destroy */
-	u64 vm_handle;
-
-	/** @exec_queue_handle: engine handle */
-	u64 exec_queue_handle;
-
-	/** @engine_handle: engine class */
-	u32 engine_class;
-
-	/** @width: submission width (number BB per exec) for this exec queue */
-	u32 width;
-
-	/** @lrc_handles: handles for each logical ring context created with this exec queue */
-	u64 lrc_handle[] __counted_by(width);
-};
-
-struct xe_eudebug_event_exec_queue_placements {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client for the engine create/destroy */
-	u64 client_handle;
-
-	/** @vm_handle: vm handle for the engine create/destroy */
-	u64 vm_handle;
-
-	/** @exec_queue_handle: engine handle */
-	u64 exec_queue_handle;
-
-	/** @engine_handle: engine class */
-	u64 lrc_handle;
-
-	/** @num_placements: all possible placements for given lrc */
-	u32 num_placements;
-
-	/** @pad: padding */
-	u32 pad;
-
-	/** @instances: num_placements sized array containing drm_xe_engine_class_instance*/
-	u64 instances[]; __counted_by(num_placements);
-};
-
-/**
- * struct xe_eudebug_event_eu_attention - Internal event for EU attention
- */
-struct xe_eudebug_event_eu_attention {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client for the attention */
-	u64 client_handle;
-
-	/** @exec_queue_handle: handle of exec_queue which raised attention */
-	u64 exec_queue_handle;
-
-	/** @lrc_handle: lrc handle of the workload which raised attention */
-	u64 lrc_handle;
-
-	/** @flags: eu attention event flags, currently MBZ */
-	u32 flags;
-
-	/** @bitmask_size: size of the bitmask, specific to device */
-	u32 bitmask_size;
-
-	/**
-	 * @bitmask: reflects threads currently signalling attention,
-	 * starting from natural hardware order of DSS=0, eu=0
-	 */
-	u8 bitmask[] __counted_by(bitmask_size);
-};
-
-/**
- * struct xe_eudebug_event_vm_bind - Internal event for vm bind/unbind operation
- */
-struct xe_eudebug_event_vm_bind {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	u64 client_handle;
-	u64 vm_handle;
-
-	u32 flags;
-	u32 num_binds;
-};
-
-struct xe_eudebug_event_vm_bind_op {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-	u64 vm_bind_ref_seqno;
-	u64 num_extensions;
-
-	u64 addr; /* Zero for unmap all ? */
-	u64 range; /* Zero for unmap all ? */
-};
-
-struct xe_eudebug_event_vm_bind_ufence {
-	struct xe_eudebug_event base;
-	u64 vm_bind_ref_seqno;
-};
-
-struct xe_eudebug_event_metadata {
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client for the attention */
-	u64 client_handle;
-
-	/** @metadata_handle: debug metadata handle it's created/destroyed */
-	u64 metadata_handle;
-
-	/* @type: metadata type, refer to xe_drm.h for options */
-	u64 type;
-
-	/* @len: size of metadata paylad */
-	u64 len;
-};
-
-struct xe_eudebug_event_vm_bind_op_metadata {
-	struct xe_eudebug_event base;
-	u64 vm_bind_op_ref_seqno;
-
-	u64 metadata_handle;
-	u64 metadata_cookie;
-};
-
-/**
- * struct xe_eudebug_event_pagefault - Internal event for EU Pagefault
- */
-struct xe_eudebug_event_pagefault {
-	/** @base: base event */
-	struct xe_eudebug_event base;
-
-	/** @client_handle: client for the Pagefault */
-	u64 client_handle;
-
-	/** @exec_queue_handle: handle of exec_queue which raised Pagefault */
-	u64 exec_queue_handle;
-
-	/** @lrc_handle: lrc handle of the workload which raised Pagefault */
-	u64 lrc_handle;
-
-	/** @flags: eu Pagefault event flags, currently MBZ */
-	u32 flags;
-
-	/**
-	 * @bitmask_size: sum of size before/after/resolved att bits.
-	 * It has three times the size of xe_eudebug_event_eu_attention.bitmask_size.
-	 */
-	u32 bitmask_size;
-
-	/** @pagefault_address: The ppgtt address where the Pagefault occurred */
-	u64 pagefault_address;
-
-	/**
-	 * @bitmask: Bitmask of thread attentions starting from natural,
-	 * hardware order of DSS=0, eu=0, 8 attention bits per eu.
-	 * The order of the bitmask array is before, after, resolved.
-	 */
-	u8 bitmask[];
-};
-
-/**
- * struct xe_eudebug_pagefault - eudebug structure for queuing pagefault
- */
-struct xe_eudebug_pagefault {
-	/** @list: link into the xe_eudebug.pagefaults */
-	struct list_head list;
-	/** @q: exec_queue which raised pagefault */
-	struct xe_exec_queue *q;
-	/** @lrc_idx: lrc index of the workload which raised pagefault */
-	int lrc_idx;
-
-	/* pagefault raw partial data passed from guc*/
-	struct {
-		/** @addr: ppgtt address where the pagefault occurred */
-		u64 addr;
-		int type;
-		int level;
-		int access;
-	} fault;
-
-	struct {
-		/** @before: state of attention bits before page fault WA processing*/
-		struct xe_eu_attentions before;
-		/**
-		 * @after: status of attention bits during page fault WA processing.
-		 * It includes eu threads where attention bits are turned on for
-		 * reasons other than page fault WA (breakpoint, interrupt, etc.).
-		 */
-		struct xe_eu_attentions after;
-		/**
-		 * @resolved: state of the attention bits after page fault WA.
-		 * It includes the eu thread that caused the page fault.
-		 * To determine the eu thread that caused the page fault,
-		 * do XOR attentions.after and attentions.resolved.
-		 */
-		struct xe_eu_attentions resolved;
-	} attentions;
-
-	/**
-	 * @deferred_resolved: to update attentions.resolved again when attention
-	 * bits are ready if the eu thread fails to turn on attention bits within
-	 * a certain time after page fault WA processing.
-	 */
-	bool deferred_resolved;
-};
-
-#endif
+#endif /*__WRAP_XE_EUDEBUG_TYPES_H_ */
diff --git a/drivers/gpu/drm/xe/xe_exec_queue.c b/drivers/gpu/drm/xe/xe_exec_queue.c
index ad9f9303171b..3fff67fce26a 100644
--- a/drivers/gpu/drm/xe/xe_exec_queue.c
+++ b/drivers/gpu/drm/xe/xe_exec_queue.c
@@ -407,7 +407,7 @@ static int exec_queue_set_eudebug(struct xe_device *xe, struct xe_exec_queue *q,
 	if (XE_IOCTL_DBG(xe, (value & ~known_flags)))
 		return -EINVAL;
 
-	if (XE_IOCTL_DBG(xe, !IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)))
+	if (XE_IOCTL_DBG(xe, !IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)))
 		return -EOPNOTSUPP;
 
 	if (XE_IOCTL_DBG(xe, !xe_exec_queue_is_lr(q)))
@@ -420,7 +420,7 @@ static int exec_queue_set_eudebug(struct xe_device *xe, struct xe_exec_queue *q,
 			 !(value & DRM_XE_EXEC_QUEUE_EUDEBUG_FLAG_ENABLE)))
 		return -EINVAL;
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	if (XE_IOCTL_DBG(xe, !xe->eudebug.enable))
 		return -EPERM;
 #endif
diff --git a/drivers/gpu/drm/xe/xe_gt_debug.h b/drivers/gpu/drm/xe/xe_gt_debug.h
index 3e3fb501eedd..8be12f54ab4c 100644
--- a/drivers/gpu/drm/xe/xe_gt_debug.h
+++ b/drivers/gpu/drm/xe/xe_gt_debug.h
@@ -1,42 +1,6 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
+#ifndef __WRAP_XE_GT_DEBUG_
+#define __WRAP_XE_GT_DEBUG_
 
-#ifndef __XE_GT_DEBUG_
-#define __XE_GT_DEBUG_
+#include "prelim/xe_gt_debug.h"
 
-#define TD_EU_ATTENTION_MAX_ROWS 2u
-
-#include "xe_gt_types.h"
-
-#define XE_GT_ATTENTION_TIMEOUT_MS 100
-
-struct xe_eu_attentions {
-#define XE_MAX_EUS 1024
-#define XE_MAX_THREADS 10
-
-	u8 att[DIV_ROUND_UP(XE_MAX_EUS * XE_MAX_THREADS, BITS_PER_BYTE)];
-	unsigned int size;
-	ktime_t ts;
-};
-
-int xe_gt_eu_threads_needing_attention(struct xe_gt *gt);
-int xe_gt_foreach_dss_group_instance(struct xe_gt *gt,
-				     int (*fn)(struct xe_gt *gt,
-					       void *data,
-					       u16 group,
-					       u16 instance),
-				     void *data);
-
-int xe_gt_eu_attention_bitmap_size(struct xe_gt *gt);
-int xe_gt_eu_attention_bitmap(struct xe_gt *gt, u8 *bits,
-			      unsigned int bitmap_size);
-
-void xe_gt_eu_attentions_read(struct xe_gt *gt,
-			      struct xe_eu_attentions *a,
-			      const unsigned int settle_time_ms);
-
-unsigned int xe_eu_attentions_xor_count(const struct xe_eu_attentions *a,
-					const struct xe_eu_attentions *b);
-#endif
+#endif /* __WRAP_XE_GT_DEBUG_ */
diff --git a/drivers/gpu/drm/xe/xe_sync_types.h b/drivers/gpu/drm/xe/xe_sync_types.h
index dcd3165e66a7..63807cfe5ba2 100644
--- a/drivers/gpu/drm/xe/xe_sync_types.h
+++ b/drivers/gpu/drm/xe/xe_sync_types.h
@@ -21,7 +21,7 @@ struct xe_user_fence {
 	u64 value;
 	int signalled;
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	struct {
 		spinlock_t lock;
 		struct xe_eudebug *debugger;
diff --git a/drivers/gpu/drm/xe/xe_vm.c b/drivers/gpu/drm/xe/xe_vm.c
index 12e626669893..7cf76611d8b8 100644
--- a/drivers/gpu/drm/xe/xe_vm.c
+++ b/drivers/gpu/drm/xe/xe_vm.c
@@ -941,7 +941,7 @@ static struct xe_vma *xe_vma_create(struct xe_vm *vm,
 			vma->gpuva.gem.obj = &bo->ttm.base;
 	}
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	INIT_LIST_HEAD(&vma->eudebug.metadata.list);
 #endif
 	INIT_LIST_HEAD(&vma->combined_links.rebind);
@@ -1999,7 +1999,7 @@ vm_bind_ioctl_ops_create(struct xe_vm *vm, struct xe_bo *bo,
 			op->map.is_null = flags & DRM_XE_VM_BIND_FLAG_NULL;
 			op->map.dumpable = flags & DRM_XE_VM_BIND_FLAG_DUMPABLE;
 			op->map.pat_index = pat_index;
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 			INIT_LIST_HEAD(&op->map.eudebug.metadata.list);
 #endif
 		} else if (__op->op == DRM_GPUVA_OP_PREFETCH) {
diff --git a/drivers/gpu/drm/xe/xe_vm_types.h b/drivers/gpu/drm/xe/xe_vm_types.h
index 1c5776194e54..72104ea0ce2b 100644
--- a/drivers/gpu/drm/xe/xe_vm_types.h
+++ b/drivers/gpu/drm/xe/xe_vm_types.h
@@ -70,7 +70,7 @@ struct xe_userptr {
 #endif
 };
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 struct xe_eudebug_vma_metadata {
 	struct list_head list;
 };
@@ -296,7 +296,7 @@ struct xe_vm {
 	/** @xef: XE file handle for tracking this VM's drm client */
 	struct xe_file *xef;
 
-#if IS_ENABLED(CONFIG_DRM_XE_EUDEBUG)
+#if IS_ENABLED(CONFIG_PRELIM_DRM_XE_EUDEBUG)
 	struct {
 		/** @lock: Lock for eudebug_bind members */
 		spinlock_t lock;
diff --git a/include/uapi/drm/xe_drm.h b/include/uapi/drm/xe_drm.h
index 430635249908..d7a0ee7f9779 100644
--- a/include/uapi/drm/xe_drm.h
+++ b/include/uapi/drm/xe_drm.h
@@ -102,9 +102,6 @@ extern "C" {
 #define DRM_XE_EXEC			0x09
 #define DRM_XE_WAIT_USER_FENCE		0x0a
 #define DRM_XE_OBSERVATION		0x0b
-#define DRM_XE_EUDEBUG_CONNECT		0x0c
-#define DRM_XE_DEBUG_METADATA_CREATE	0x0d
-#define DRM_XE_DEBUG_METADATA_DESTROY	0x0e
 /* Must be kept compact -- no holes */
 
 #define DRM_IOCTL_XE_DEVICE_QUERY		DRM_IOWR(DRM_COMMAND_BASE + DRM_XE_DEVICE_QUERY, struct drm_xe_device_query)
@@ -119,9 +116,6 @@ extern "C" {
 #define DRM_IOCTL_XE_EXEC			DRM_IOW(DRM_COMMAND_BASE + DRM_XE_EXEC, struct drm_xe_exec)
 #define DRM_IOCTL_XE_WAIT_USER_FENCE		DRM_IOWR(DRM_COMMAND_BASE + DRM_XE_WAIT_USER_FENCE, struct drm_xe_wait_user_fence)
 #define DRM_IOCTL_XE_OBSERVATION		DRM_IOW(DRM_COMMAND_BASE + DRM_XE_OBSERVATION, struct drm_xe_observation_param)
-#define DRM_IOCTL_XE_EUDEBUG_CONNECT		DRM_IOWR(DRM_COMMAND_BASE + DRM_XE_EUDEBUG_CONNECT, struct drm_xe_eudebug_connect)
-#define DRM_IOCTL_XE_DEBUG_METADATA_CREATE	 DRM_IOWR(DRM_COMMAND_BASE + DRM_XE_DEBUG_METADATA_CREATE, struct drm_xe_debug_metadata_create)
-#define DRM_IOCTL_XE_DEBUG_METADATA_DESTROY	 DRM_IOW(DRM_COMMAND_BASE + DRM_XE_DEBUG_METADATA_DESTROY, struct drm_xe_debug_metadata_destroy)
 
 /**
  * DOC: Xe IOCTL Extensions
@@ -886,23 +880,6 @@ struct drm_xe_vm_destroy {
 	__u64 reserved[2];
 };
 
-struct drm_xe_vm_bind_op_ext_attach_debug {
-	/** @base: base user extension */
-	struct drm_xe_user_extension base;
-
-	/** @id: Debug object id from create metadata */
-	__u64 metadata_id;
-
-	/** @flags: Flags */
-	__u64 flags;
-
-	/** @cookie: Cookie */
-	__u64 cookie;
-
-	/** @reserved: Reserved */
-	__u64 reserved;
-};
-
 /**
  * struct drm_xe_vm_bind_op - run bind operations
  *
@@ -929,7 +906,6 @@ struct drm_xe_vm_bind_op_ext_attach_debug {
  */
 
 struct drm_xe_vm_bind_op {
-#define XE_VM_BIND_OP_EXTENSIONS_ATTACH_DEBUG 0
 	/** @extensions: Pointer to the first extension struct, if any */
 	__u64 extensions;
 
@@ -1132,8 +1108,6 @@ struct drm_xe_exec_queue_create {
 #define DRM_XE_EXEC_QUEUE_EXTENSION_SET_PROPERTY		0
 #define   DRM_XE_EXEC_QUEUE_SET_PROPERTY_PRIORITY		0
 #define   DRM_XE_EXEC_QUEUE_SET_PROPERTY_TIMESLICE		1
-#define   DRM_XE_EXEC_QUEUE_SET_PROPERTY_EUDEBUG		2
-#define     DRM_XE_EXEC_QUEUE_EUDEBUG_FLAG_ENABLE		(1 << 0)
 	/** @extensions: Pointer to the first extension struct, if any */
 	__u64 extensions;
 
@@ -1719,72 +1693,6 @@ struct drm_xe_oa_stream_info {
 	__u64 reserved[3];
 };
 
-/*
- * Debugger ABI (ioctl and events) Version History:
- * 0 - No debugger available
- * 1 - Initial version
- */
-#define DRM_XE_EUDEBUG_VERSION 1
-
-struct drm_xe_eudebug_connect {
-	/** @extensions: Pointer to the first extension struct, if any */
-	__u64 extensions;
-
-	__u64 pid; /* input: Target process ID */
-	__u32 flags; /* MBZ */
-
-	__u32 version; /* output: current ABI (ioctl / events) version */
-};
-
-/*
- * struct drm_xe_debug_metadata_create - Create debug metadata
- *
- * Add a region of user memory to be marked as debug metadata.
- * When the debugger attaches, the metadata regions will be delivered
- * for debugger. Debugger can then map these regions to help decode
- * the program state.
- *
- * Returns handle to created metadata entry.
- */
-struct drm_xe_debug_metadata_create {
-	/** @extensions: Pointer to the first extension struct, if any */
-	__u64 extensions;
-
-#define DRM_XE_DEBUG_METADATA_ELF_BINARY     0
-#define DRM_XE_DEBUG_METADATA_PROGRAM_MODULE 1
-#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_MODULE_AREA 2
-#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SBA_AREA 3
-#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA 4
-#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_NUM (1 + \
-	  WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA)
-
-	/** @type: Type of metadata */
-	__u64 type;
-
-	/** @user_addr: pointer to start of the metadata */
-	__u64 user_addr;
-
-	/** @len: length, in bytes of the medata */
-	__u64 len;
-
-	/** @metadata_id: created metadata handle (out) */
-	__u32 metadata_id;
-};
-
-/**
- * struct drm_xe_debug_metadata_destroy - Destroy debug metadata
- *
- * Destroy debug metadata.
- */
-struct drm_xe_debug_metadata_destroy {
-	/** @extensions: Pointer to the first extension struct, if any */
-	__u64 extensions;
-
-	/** @metadata_id: metadata handle to destroy */
-	__u32 metadata_id;
-};
-
-#include "xe_drm_eudebug.h"
 #include "xe_drm_prelim.h"
 
 #if defined(__cplusplus)
diff --git a/include/uapi/drm/xe_drm_eudebug.h b/include/uapi/drm/xe_drm_eudebug.h
deleted file mode 100644
index e43576c7bc5e..000000000000
--- a/include/uapi/drm/xe_drm_eudebug.h
+++ /dev/null
@@ -1,256 +0,0 @@
-/* SPDX-License-Identifier: MIT */
-/*
- * Copyright © 2023 Intel Corporation
- */
-
-#ifndef _UAPI_XE_DRM_EUDEBUG_H_
-#define _UAPI_XE_DRM_EUDEBUG_H_
-
-#if defined(__cplusplus)
-extern "C" {
-#endif
-
-/**
- * Do a eudebug event read for a debugger connection.
- *
- * This ioctl is available in debug version 1.
- */
-#define DRM_XE_EUDEBUG_IOCTL_READ_EVENT		_IO('j', 0x0)
-#define DRM_XE_EUDEBUG_IOCTL_EU_CONTROL		_IOWR('j', 0x2, struct drm_xe_eudebug_eu_control)
-#define DRM_XE_EUDEBUG_IOCTL_ACK_EVENT		_IOW('j', 0x4, struct drm_xe_eudebug_ack_event)
-#define DRM_XE_EUDEBUG_IOCTL_VM_OPEN		_IOW('j', 0x1, struct drm_xe_eudebug_vm_open)
-#define DRM_XE_EUDEBUG_IOCTL_READ_METADATA	_IOWR('j', 0x3, struct drm_xe_eudebug_read_metadata)
-
-/* XXX: Document events to match their internal counterparts when moved to xe_drm.h */
-struct drm_xe_eudebug_event {
-	__u32 len;
-
-	__u16 type;
-#define DRM_XE_EUDEBUG_EVENT_NONE		0
-#define DRM_XE_EUDEBUG_EVENT_READ		1
-#define DRM_XE_EUDEBUG_EVENT_OPEN		2
-#define DRM_XE_EUDEBUG_EVENT_VM			3
-#define DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE		4
-#define DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE_PLACEMENTS 5
-#define DRM_XE_EUDEBUG_EVENT_EU_ATTENTION	6
-#define DRM_XE_EUDEBUG_EVENT_VM_BIND		7
-#define DRM_XE_EUDEBUG_EVENT_VM_BIND_OP		8
-#define DRM_XE_EUDEBUG_EVENT_VM_BIND_UFENCE	9
-#define DRM_XE_EUDEBUG_EVENT_METADATA		10
-#define DRM_XE_EUDEBUG_EVENT_VM_BIND_OP_METADATA 11
-#define DRM_XE_EUDEBUG_EVENT_PAGEFAULT		12
-
-	__u16 flags;
-#define DRM_XE_EUDEBUG_EVENT_CREATE		(1 << 0)
-#define DRM_XE_EUDEBUG_EVENT_DESTROY		(1 << 1)
-#define DRM_XE_EUDEBUG_EVENT_STATE_CHANGE	(1 << 2)
-#define DRM_XE_EUDEBUG_EVENT_NEED_ACK		(1 << 3)
-
-	__u64 seqno;
-	__u64 reserved;
-};
-
-struct drm_xe_eudebug_event_client {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle; /* This is unique per debug connection */
-};
-
-struct drm_xe_eudebug_event_vm {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 vm_handle;
-};
-
-struct drm_xe_eudebug_event_exec_queue {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 vm_handle;
-	__u64 exec_queue_handle;
-	__u32 engine_class;
-	__u32 width;
-	__u64 lrc_handle[];
-};
-
-struct drm_xe_eudebug_event_exec_queue_placements {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 vm_handle;
-	__u64 exec_queue_handle;
-	__u64 lrc_handle;
-	__u32 num_placements;
-	__u32 pad;
-	/**
-	 * @instances: user pointer to num_placements sized array of struct
-	 * drm_xe_engine_class_instance
-	 */
-	__u64 instances[];
-};
-
-struct drm_xe_eudebug_event_eu_attention {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 exec_queue_handle;
-	__u64 lrc_handle;
-	__u32 flags;
-	__u32 bitmask_size;
-	__u8 bitmask[];
-};
-
-struct drm_xe_eudebug_eu_control {
-	__u64 client_handle;
-
-#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_INTERRUPT_ALL	0
-#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_STOPPED		1
-#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_RESUME		2
-	__u32 cmd;
-	__u32 flags;
-
-	__u64 seqno;
-
-	__u64 exec_queue_handle;
-	__u64 lrc_handle;
-	__u32 reserved;
-	__u32 bitmask_size;
-	__u64 bitmask_ptr;
-};
-
-/*
- *  When client (debuggee) does vm_bind_ioctl() following event
- *  sequence will be created (for the debugger):
- *
- *  ┌───────────────────────┐
- *  │  EVENT_VM_BIND        ├───────┬─┬─┐
- *  └───────────────────────┘       │ │ │
- *      ┌───────────────────────┐   │ │ │
- *      │ EVENT_VM_BIND_OP #1   ├───┘ │ │
- *      └───────────────────────┘     │ │
- *                 ...                │ │
- *      ┌───────────────────────┐     │ │
- *      │ EVENT_VM_BIND_OP #n   ├─────┘ │
- *      └───────────────────────┘       │
- *                                      │
- *      ┌───────────────────────┐       │
- *      │ EVENT_UFENCE          ├───────┘
- *      └───────────────────────┘
- *
- * All the events below VM_BIND will reference the VM_BIND
- * they associate with, by field .vm_bind_ref_seqno.
- * event_ufence will only be included if the client did
- * attach sync of type UFENCE into its vm_bind_ioctl().
- *
- * When EVENT_UFENCE is sent by the driver, all the OPs of
- * the original VM_BIND are completed and the [addr,range]
- * contained in them are present and modifiable through the
- * vm accessors. Accessing [addr, range] before related ufence
- * event will lead to undefined results as the actual bind
- * operations are async and the backing storage might not
- * be there on a moment of receiving the event.
- *
- * Client's UFENCE sync will be held by the driver: client's
- * drm_xe_wait_ufence will not complete and the value of the ufence
- * won't appear until ufence is acked by the debugger process calling
- * DRM_XE_EUDEBUG_IOCTL_ACK_EVENT with the event_ufence.base.seqno.
- * This will signal the fence, .value will update and the wait will
- * complete allowing the client to continue.
- *
- */
-
-struct drm_xe_eudebug_event_vm_bind {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 vm_handle;
-
-	__u32 flags;
-#define DRM_XE_EUDEBUG_EVENT_VM_BIND_FLAG_UFENCE (1 << 0)
-
-	__u32 num_binds;
-};
-
-struct drm_xe_eudebug_event_vm_bind_op {
-	struct drm_xe_eudebug_event base;
-	__u64 vm_bind_ref_seqno; /* *_event_vm_bind.base.seqno */
-	__u64 num_extensions;
-
-	__u64 addr; /* XXX: Zero for unmap all? */
-	__u64 range; /* XXX: Zero for unmap all? */
-};
-
-struct drm_xe_eudebug_event_vm_bind_ufence {
-	struct drm_xe_eudebug_event base;
-	__u64 vm_bind_ref_seqno; /* *_event_vm_bind.base.seqno */
-};
-
-struct drm_xe_eudebug_ack_event {
-	__u32 type;
-	__u32 flags; /* MBZ */
-	__u64 seqno;
-};
-
-struct drm_xe_eudebug_vm_open {
-	/** @extensions: Pointer to the first extension struct, if any */
-	__u64 extensions;
-
-	/** @client_handle: id of client */
-	__u64 client_handle;
-
-	/** @vm_handle: id of vm */
-	__u64 vm_handle;
-
-	/** @flags: flags */
-	__u64 flags;
-
-#define DRM_XE_EUDEBUG_VM_SYNC_MAX_TIMEOUT_NSECS (10ULL * NSEC_PER_SEC)
-	/** @timeout_ns: Timeout value in nanoseconds operations (fsync) */
-	__u64 timeout_ns;
-};
-
-struct drm_xe_eudebug_read_metadata {
-	__u64 client_handle;
-	__u64 metadata_handle;
-	__u32 flags;
-	__u32 reserved;
-	__u64 ptr;
-	__u64 size;
-};
-
-struct drm_xe_eudebug_event_metadata {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 metadata_handle;
-	/* XXX: Refer to xe_drm.h for fields */
-	__u64 type;
-	__u64 len;
-};
-
-struct drm_xe_eudebug_event_vm_bind_op_metadata {
-	struct drm_xe_eudebug_event base;
-	__u64 vm_bind_op_ref_seqno; /* *_event_vm_bind_op.base.seqno */
-
-	__u64 metadata_handle;
-	__u64 metadata_cookie;
-};
-
-struct drm_xe_eudebug_event_pagefault {
-	struct drm_xe_eudebug_event base;
-
-	__u64 client_handle;
-	__u64 exec_queue_handle;
-	__u64 lrc_handle;
-	__u32 flags;
-	__u32 bitmask_size;
-	__u64 pagefault_address;
-	__u8 bitmask[];
-};
-
-#if defined(__cplusplus)
-}
-#endif
-
-#endif
diff --git a/include/uapi/drm/xe_drm_prelim.h b/include/uapi/drm/xe_drm_prelim.h
index 9c86f5969500..a0825834c878 100644
--- a/include/uapi/drm/xe_drm_prelim.h
+++ b/include/uapi/drm/xe_drm_prelim.h
@@ -70,4 +70,416 @@
  * components. Please add an unreserved ioctl number here to reserve that
  * number.
  */
+/* SPDX-License-Identifier: MIT */
+/*
+ * Copyright © 2023 Intel Corporation
+ */
+
+#define PRELIM_PREFIX_MACRO(__sym) PRELIM_ ##__sym
+#define PRELIM_PREFIX(__sym) prelim_ ## __sym
+
+#define PRELIM_DRM_XE_EUDEBUG_CONNECT 0x5f
+#define DRM_XE_EUDEBUG_CONNECT PRELIM_DRM_XE_EUDEBUG_CONNECT
+#define PRELIM_DRM_XE_DEBUG_METADATA_CREATE 0x5e
+#define DRM_XE_DEBUG_METADATA_CREATE PRELIM_DRM_XE_DEBUG_METADATA_CREATE
+#define PRELIM_DRM_XE_DEBUG_METADATA_DESTROY 0x5d
+#define DRM_XE_DEBUG_METADATA_DESTROY PRELIM_DRM_XE_DEBUG_METADATA_DESTROY
+
+#define drm_xe_eudebug_connect PRELIM_PREFIX(drm_xe_eudebug_connect)
+
+#define PRELIM_DRM_IOCTL_XE_EUDEBUG_CONNECT		DRM_IOWR(DRM_COMMAND_BASE + DRM_XE_EUDEBUG_CONNECT, struct drm_xe_eudebug_connect)
+#define DRM_IOCTL_XE_EUDEBUG_CONNECT PRELIM_PREFIX_MACRO(DRM_IOCTL_XE_EUDEBUG_CONNECT)
+#define PRELIM_DRM_IOCTL_XE_DEBUG_METADATA_CREATE	 DRM_IOWR(DRM_COMMAND_BASE +DRM_XE_DEBUG_METADATA_CREATE, struct drm_xe_debug_metadata_create)
+#define DRM_IOCTL_XE_DEBUG_METADATA_CREATE PRELIM_PREFIX_MACRO(DRM_IOCTL_XE_DEBUG_METADATA_CREATE)
+#define PRELIM_DRM_IOCTL_XE_DEBUG_METADATA_DESTROY	 DRM_IOW(DRM_COMMAND_BASE + DRM_XE_DEBUG_METADATA_DESTROY, struct drm_xe_debug_metadata_destroy)
+#define DRM_IOCTL_XE_DEBUG_METADATA_DESTROY PRELIM_PREFIX_MACRO(DRM_IOCTL_XE_DEBUG_METADATA_DESTROY)
+
+#define drm_xe_vm_bind_op_ext_attach_debug PRELIM_PREFIX(drm_xe_vm_bind_op_ext_attach_debug)
+struct drm_xe_vm_bind_op_ext_attach_debug {
+	/** @base: base user extension */
+	struct drm_xe_user_extension base;
+
+	/** @id: Debug object id from create metadata */
+	__u64 metadata_id;
+
+	/** @flags: Flags */
+	__u64 flags;
+
+	/** @cookie: Cookie */
+	__u64 cookie;
+
+	/** @reserved: Reserved */
+	__u64 reserved;
+};
+
+#define PRELIM_XE_VM_BIND_OP_EXTENSIONS_ATTACH_DEBUG 0
+#define XE_VM_BIND_OP_EXTENSIONS_ATTACH_DEBUG PRELIM_PREFIX_MACRO(XE_VM_BIND_OP_EXTENSIONS_ATTACH_DEBUG)
+
+#define   PRELIM_DRM_XE_EXEC_QUEUE_SET_PROPERTY_EUDEBUG		2
+#define   DRM_XE_EXEC_QUEUE_SET_PROPERTY_EUDEBUG PRELIM_PREFIX_MACRO(DRM_XE_EXEC_QUEUE_SET_PROPERTY_EUDEBUG)
+#define     PRELIM_DRM_XE_EXEC_QUEUE_EUDEBUG_FLAG_ENABLE		(1 << 0)
+#define     DRM_XE_EXEC_QUEUE_EUDEBUG_FLAG_ENABLE PRELIM_PREFIX_MACRO(DRM_XE_EXEC_QUEUE_EUDEBUG_FLAG_ENABLE)
+
+/*
+ * Debugger ABI (ioctl and events) Version History:
+ * 0 - No debugger available
+ * 1 - Initial version
+ */
+#define PRELIM_DRM_XE_EUDEBUG_VERSION 1
+#define DRM_XE_EUDEBUG_VERSION PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_VERSION)
+
+#define drm_xe_eudebug_connect PRELIM_PREFIX(drm_xe_eudebug_connect)
+struct drm_xe_eudebug_connect {
+	/** @extensions: Pointer to the first extension struct, if any */
+	__u64 extensions;
+
+	__u64 pid; /* input: Target process ID */
+	__u32 flags; /* MBZ */
+
+	__u32 version; /* output: current ABI (ioctl / events) version */
+};
+
+/*
+ * struct drm_xe_debug_metadata_create - Create debug metadata
+ *
+ * Add a region of user memory to be marked as debug metadata.
+ * When the debugger attaches, the metadata regions will be delivered
+ * for debugger. Debugger can then map these regions to help decode
+ * the program state.
+ *
+ * Returns handle to created metadata entry.
+ */
+#define drm_xe_debug_metadata_create PRELIM_PREFIX(drm_xe_debug_metadata_create)
+struct drm_xe_debug_metadata_create {
+	/** @extensions: Pointer to the first extension struct, if any */
+	__u64 extensions;
+
+#define DRM_XE_DEBUG_METADATA_ELF_BINARY PRELIM_PREFIX_MACRO(DRM_XE_DEBUG_METADATA_ELF_BINARY)
+#define DRM_XE_DEBUG_METADATA_PROGRAM_MODULE PRELIM_PREFIX_MACRO(DRM_XE_DEBUG_METADATA_PROGRAM_MODULE)
+#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_MODULE_AREA PRELIM_PREFIX_MACRO(WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_MODULE_AREA)
+#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SBA_AREA PRELIM_PREFIX_MACRO(WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SBA_AREA)
+#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA PRELIM_WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA
+#define WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_NUM PRELIM_PREFIX_MACRO(WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_NUM)
+
+#define PRELIM_DRM_XE_DEBUG_METADATA_ELF_BINARY     0
+#define PRELIM_DRM_XE_DEBUG_METADATA_PROGRAM_MODULE 1
+#define PRELIM_WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_MODULE_AREA 2
+#define PRELIM_WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SBA_AREA 3
+#define PRELIM_WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA 4
+#define PRELIM_WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_NUM (1 + \
+	  WORK_IN_PROGRESS_DRM_XE_DEBUG_METADATA_SIP_AREA)
+
+	/** @type: Type of metadata */
+	__u64 type;
+
+	/** @user_addr: pointer to start of the metadata */
+	__u64 user_addr;
+
+	/** @len: length, in bytes of the medata */
+	__u64 len;
+
+	/** @metadata_id: created metadata handle (out) */
+	__u32 metadata_id;
+};
+
+/**
+ * struct drm_xe_debug_metadata_destroy - Destroy debug metadata
+ *
+ * Destroy debug metadata.
+ */
+#define drm_xe_debug_metadata_destroy PRELIM_PREFIX(drm_xe_debug_metadata_destroy)
+struct drm_xe_debug_metadata_destroy {
+	/** @extensions: Pointer to the first extension struct, if any */
+	__u64 extensions;
+
+	/** @metadata_id: metadata handle to destroy */
+	__u32 metadata_id;
+};
+
+/**
+ * Do a eudebug event read for a debugger connection.
+ *
+ * This ioctl is available in debug version 1.
+ */
+#define PRELIM_DRM_XE_EUDEBUG_IOCTL_READ_EVENT		_IO('j', 0x0)
+#define PRELIM_DRM_XE_EUDEBUG_IOCTL_EU_CONTROL		_IOWR('j', 0x2, struct drm_xe_eudebug_eu_control)
+#define PRELIM_DRM_XE_EUDEBUG_IOCTL_ACK_EVENT		_IOW('j', 0x4, struct drm_xe_eudebug_ack_event)
+#define PRELIM_DRM_XE_EUDEBUG_IOCTL_VM_OPEN		_IOW('j', 0x1, struct drm_xe_eudebug_vm_open)
+#define PRELIM_DRM_XE_EUDEBUG_IOCTL_READ_METADATA	_IOWR('j', 0x3, struct drm_xe_eudebug_read_metadata)
+
+#define DRM_XE_EUDEBUG_IOCTL_READ_EVENT 	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_IOCTL_READ_EVENT)
+#define DRM_XE_EUDEBUG_IOCTL_EU_CONTROL		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_IOCTL_EU_CONTROL)
+#define DRM_XE_EUDEBUG_IOCTL_ACK_EVENT		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_IOCTL_ACK_EVENT)
+#define DRM_XE_EUDEBUG_IOCTL_VM_OPEN		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_IOCTL_VM_OPEN)
+#define DRM_XE_EUDEBUG_IOCTL_READ_METADATA	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_IOCTL_READ_METADATA)
+
+/* XXX: Document events to match their internal counterparts when moved to xe_drm.h */
+#define drm_xe_eudebug_event PRELIM_PREFIX(drm_xe_eudebug_event)
+struct drm_xe_eudebug_event {
+	__u32 len;
+
+	__u16 type;
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_NONE		0
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_READ		1
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_OPEN		2
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM			3
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE		4
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE_PLACEMENTS 5
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_EU_ATTENTION	6
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM_BIND		7
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM_BIND_OP		8
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM_BIND_UFENCE	9
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_METADATA		10
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM_BIND_OP_METADATA 11
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_PAGEFAULT		12
+
+#define DRM_XE_EUDEBUG_EVENT_NONE			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_NONE)
+#define DRM_XE_EUDEBUG_EVENT_READ			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_READ)
+#define DRM_XE_EUDEBUG_EVENT_OPEN			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_OPEN)
+#define DRM_XE_EUDEBUG_EVENT_VM				PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM)
+#define DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE)
+#define DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE_PLACEMENTS 	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_EXEC_QUEUE_PLACEMENTS)
+#define DRM_XE_EUDEBUG_EVENT_EU_ATTENTION		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_EU_ATTENTION)
+#define DRM_XE_EUDEBUG_EVENT_VM_BIND			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM_BIND)
+#define DRM_XE_EUDEBUG_EVENT_VM_BIND_OP			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM_BIND_OP)
+#define DRM_XE_EUDEBUG_EVENT_VM_BIND_UFENCE		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM_BIND_UFENCE)
+#define DRM_XE_EUDEBUG_EVENT_METADATA			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_METADATA)
+#define DRM_XE_EUDEBUG_EVENT_VM_BIND_OP_METADATA 	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM_BIND_OP_METADATA)
+#define DRM_XE_EUDEBUG_EVENT_PAGEFAULT			PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_PAGEFAULT)
+
+	__u16 flags;
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_CREATE		(1 << 0)
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_DESTROY		(1 << 1)
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_STATE_CHANGE	(1 << 2)
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_NEED_ACK		(1 << 3)
+
+#define DRM_XE_EUDEBUG_EVENT_CREATE		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_CREATE)
+#define DRM_XE_EUDEBUG_EVENT_DESTROY		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_DESTROY)
+#define DRM_XE_EUDEBUG_EVENT_STATE_CHANGE	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_STATE_CHANGE)
+#define DRM_XE_EUDEBUG_EVENT_NEED_ACK		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_NEED_ACK)
+
+	__u64 seqno;
+	__u64 reserved;
+};
+
+#define drm_xe_eudebug_event_client PRELIM_PREFIX(drm_xe_eudebug_event_client)
+struct drm_xe_eudebug_event_client {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle; /* This is unique per debug connection */
+};
+
+#define drm_xe_eudebug_event_vm PRELIM_PREFIX(drm_xe_eudebug_event_vm)
+struct drm_xe_eudebug_event_vm {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 vm_handle;
+};
+
+#define drm_xe_eudebug_event_exec_queue PRELIM_PREFIX(drm_xe_eudebug_event_exec_queue)
+struct drm_xe_eudebug_event_exec_queue {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 vm_handle;
+	__u64 exec_queue_handle;
+	__u32 engine_class;
+	__u32 width;
+	__u64 lrc_handle[];
+};
+
+#define drm_xe_eudebug_event_exec_queue_placements PRELIM_PREFIX(drm_xe_eudebug_event_exec_queue_placements)
+struct drm_xe_eudebug_event_exec_queue_placements {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 vm_handle;
+	__u64 exec_queue_handle;
+	__u64 lrc_handle;
+	__u32 num_placements;
+	__u32 pad;
+	/**
+	 * @instances: user pointer to num_placements sized array of struct
+	 * drm_xe_engine_class_instance
+	 */
+	__u64 instances[];
+};
+
+#define drm_xe_eudebug_event_eu_attention PRELIM_PREFIX(drm_xe_eudebug_event_eu_attention)
+struct drm_xe_eudebug_event_eu_attention {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 exec_queue_handle;
+	__u64 lrc_handle;
+	__u32 flags;
+	__u32 bitmask_size;
+	__u8 bitmask[];
+};
+
+#define drm_xe_eudebug_eu_control PRELIM_PREFIX(drm_xe_eudebug_eu_control)
+struct drm_xe_eudebug_eu_control {
+	__u64 client_handle;
+
+#define PRELIM_DRM_XE_EUDEBUG_EU_CONTROL_CMD_INTERRUPT_ALL	0
+#define PRELIM_DRM_XE_EUDEBUG_EU_CONTROL_CMD_STOPPED		1
+#define PRELIM_DRM_XE_EUDEBUG_EU_CONTROL_CMD_RESUME		2
+
+#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_INTERRUPT_ALL	PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EU_CONTROL_CMD_INTERRUPT_ALL)
+#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_STOPPED		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EU_CONTROL_CMD_STOPPED)
+#define DRM_XE_EUDEBUG_EU_CONTROL_CMD_RESUME		PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EU_CONTROL_CMD_RESUME)
+	__u32 cmd;
+	__u32 flags;
+
+	__u64 seqno;
+
+	__u64 exec_queue_handle;
+	__u64 lrc_handle;
+	__u32 reserved;
+	__u32 bitmask_size;
+	__u64 bitmask_ptr;
+};
+
+/*
+ *  When client (debuggee) does vm_bind_ioctl() following event
+ *  sequence will be created (for the debugger):
+ *
+ *  ┌───────────────────────┐
+ *  │  EVENT_VM_BIND        ├───────┬─┬─┐
+ *  └───────────────────────┘       │ │ │
+ *      ┌───────────────────────┐   │ │ │
+ *      │ EVENT_VM_BIND_OP #1   ├───┘ │ │
+ *      └───────────────────────┘     │ │
+ *                 ...                │ │
+ *      ┌───────────────────────┐     │ │
+ *      │ EVENT_VM_BIND_OP #n   ├─────┘ │
+ *      └───────────────────────┘       │
+ *                                      │
+ *      ┌───────────────────────┐       │
+ *      │ EVENT_UFENCE          ├───────┘
+ *      └───────────────────────┘
+ *
+ * All the events below VM_BIND will reference the VM_BIND
+ * they associate with, by field .vm_bind_ref_seqno.
+ * event_ufence will only be included if the client did
+ * attach sync of type UFENCE into its vm_bind_ioctl().
+ *
+ * When EVENT_UFENCE is sent by the driver, all the OPs of
+ * the original VM_BIND are completed and the [addr,range]
+ * contained in them are present and modifiable through the
+ * vm accessors. Accessing [addr, range] before related ufence
+ * event will lead to undefined results as the actual bind
+ * operations are async and the backing storage might not
+ * be there on a moment of receiving the event.
+ *
+ * Client's UFENCE sync will be held by the driver: client's
+ * drm_xe_wait_ufence will not complete and the value of the ufence
+ * won't appear until ufence is acked by the debugger process calling
+ * DRM_XE_EUDEBUG_IOCTL_ACK_EVENT with the event_ufence.base.seqno.
+ * This will signal the fence, .value will update and the wait will
+ * complete allowing the client to continue.
+ *
+ */
+
+#define drm_xe_eudebug_event_vm_bind PRELIM_PREFIX(drm_xe_eudebug_event_vm_bind)
+struct drm_xe_eudebug_event_vm_bind {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 vm_handle;
+
+	__u32 flags;
+#define PRELIM_DRM_XE_EUDEBUG_EVENT_VM_BIND_FLAG_UFENCE (1 << 0)
+#define DRM_XE_EUDEBUG_EVENT_VM_BIND_FLAG_UFENCE PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_EVENT_VM_BIND_FLAG_UFENCE)
+
+	__u32 num_binds;
+};
+
+#define drm_xe_eudebug_event_vm_bind_op PRELIM_PREFIX(drm_xe_eudebug_event_vm_bind_op)
+struct drm_xe_eudebug_event_vm_bind_op {
+	struct drm_xe_eudebug_event base;
+	__u64 vm_bind_ref_seqno; /* *_event_vm_bind.base.seqno */
+	__u64 num_extensions;
+
+	__u64 addr; /* XXX: Zero for unmap all? */
+	__u64 range; /* XXX: Zero for unmap all? */
+};
+
+#define drm_xe_eudebug_event_vm_bind_ufence PRELIM_PREFIX(drm_xe_eudebug_event_vm_bind_ufence)
+struct drm_xe_eudebug_event_vm_bind_ufence {
+	struct drm_xe_eudebug_event base;
+	__u64 vm_bind_ref_seqno; /* *_event_vm_bind.base.seqno */
+};
+
+#define drm_xe_eudebug_ack_event PRELIM_PREFIX(drm_xe_eudebug_ack_event)
+struct drm_xe_eudebug_ack_event {
+	__u32 type;
+	__u32 flags; /* MBZ */
+	__u64 seqno;
+};
+
+#define drm_xe_eudebug_vm_open PRELIM_PREFIX(drm_xe_eudebug_vm_open)
+struct drm_xe_eudebug_vm_open {
+	/** @extensions: Pointer to the first extension struct, if any */
+	__u64 extensions;
+
+	/** @client_handle: id of client */
+	__u64 client_handle;
+
+	/** @vm_handle: id of vm */
+	__u64 vm_handle;
+
+	/** @flags: flags */
+	__u64 flags;
+
+#define PRELIM_DRM_XE_EUDEBUG_VM_SYNC_MAX_TIMEOUT_NSECS (10ULL * NSEC_PER_SEC)
+#define DRM_XE_EUDEBUG_VM_SYNC_MAX_TIMEOUT_NSECS PRELIM_PREFIX_MACRO(DRM_XE_EUDEBUG_VM_SYNC_MAX_TIMEOUT_NSECS)
+	/** @timeout_ns: Timeout value in nanoseconds operations (fsync) */
+	__u64 timeout_ns;
+};
+
+#define drm_xe_eudebug_read_metadata PRELIM_PREFIX(drm_xe_eudebug_read_metadata)
+struct drm_xe_eudebug_read_metadata {
+	__u64 client_handle;
+	__u64 metadata_handle;
+	__u32 flags;
+	__u32 reserved;
+	__u64 ptr;
+	__u64 size;
+};
+
+#define drm_xe_eudebug_event_metadata PRELIM_PREFIX(drm_xe_eudebug_event_metadata)
+struct drm_xe_eudebug_event_metadata {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 metadata_handle;
+	/* XXX: Refer to xe_drm.h for fields */
+	__u64 type;
+	__u64 len;
+};
+
+#define drm_xe_eudebug_event_vm_bind_op_metadata PRELIM_PREFIX(drm_xe_eudebug_event_vm_bind_op_metadata)
+struct drm_xe_eudebug_event_vm_bind_op_metadata {
+	struct drm_xe_eudebug_event base;
+	__u64 vm_bind_op_ref_seqno; /* *_event_vm_bind_op.base.seqno */
+
+	__u64 metadata_handle;
+	__u64 metadata_cookie;
+};
+
+#define drm_xe_eudebug_event_pagefault PRELIM_PREFIX(drm_xe_eudebug_event_pagefault)
+struct drm_xe_eudebug_event_pagefault {
+	struct drm_xe_eudebug_event base;
+
+	__u64 client_handle;
+	__u64 exec_queue_handle;
+	__u64 lrc_handle;
+	__u32 flags;
+	__u32 bitmask_size;
+	__u64 pagefault_address;
+	__u8 bitmask[];
+};
+
 #endif /* _UAPI_XE_DRM_PRELIM_H_ */
-- 
2.25.1

